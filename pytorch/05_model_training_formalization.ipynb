{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vision - Training formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "[Vision Workshop](https://github.com/mblanc/vision-workshop) is a series of labs on how to build an image classification system on Google Cloud. Throughout the Vision Workshop labs, you will learn how to read image data stored in data lake, perform exploratory data analysis (EDA), train a model, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to pull features from Feature Store for training, run data exploratory analysis on features, build a machine learning model locally, experiment with various hyperparameters, evaluate the model and deloy it to a Vertex AI endpoint. \n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "- Do some exploratory analysis on the extracted data\n",
    "- Train the model and track the results using Vertex AI Training\n",
    "- Deploy the Model using Vertex AI Prediction\n",
    "- Launch a Batch predictions job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook.### Load config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-vision-workshop\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "END_DATE_TRAIN = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "## Custom Training\n",
    "DATASET_NAME=f\"sample_train-{ID}-{END_DATE_TRAIN}\"\n",
    "TRAIN_JOB_NAME=f\"vision_train_frmlz-{ID}\"\n",
    "MODEL_NAME=f\"vision_model_frmlz-{ID}\"\n",
    "DEPLOYED_NAME = f\"vision_prediction_frmlz-{ID}\"\n",
    "MODEL_SERVING_IMAGE_URI = \"europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    "IMAGE_REPOSITORY = f\"vision-{ID}\"\n",
    "IMAGE_NAME=\"image-classifier-pytorch\"\n",
    "IMAGE_TAG=\"v1\"\n",
    "IMAGE_URI=f\"europe-west4-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "TRAIN_COMPUTE=\"e2-standard-4\"\n",
    "DEPLOY_COMPUTE=\"n1-standard-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builing a custom Image Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a custom model\n",
    "In this section, we will use the tensorflow algorithm. Specifically, we will perform custom training with a custom tensorflow container.\n",
    "\n",
    "#### Create the training application\n",
    "Typically, to perform custom training you can use either a pre-built container or buid your own container. In this section we will build a container for tensorflow, and use it to train a model with the Vertex AI Managed Training service.\n",
    "\n",
    "The first step is to write your training code. Then, write a Dockerfile and build a container image based on it. The following cell, writes our code into `train_pytorch.py` which is the module for training a Tensorflow model. We will copy this code later into our container to be run through Vertex Training service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p -m 777 build_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some noteable steps include:\n",
    "\n",
    "- Command-line arguments:\n",
    "    - `train-data-dir`, `val-data-dir`, `test-data-dir`: The Cloud Storage locations of the train, validation and test data. When using Vertex AI custom training, these locations will be specified in the corresponding environment variables: `AIP_TRAINING_DATA_URI`, `AIP_VALIDATION_DATA_URI`, and `AIP_TEST_DATA_URI`. The data is exported from an `ImageDataSet` and will be in a JSONL format.\n",
    "    - `model-dir`: The location to save the trained model. When using Vertex AI custom training, the location will be specified in the environment variable: `AIP_MODEL_DIR`,\n",
    "- Data preprocessing (`get_data()`):\n",
    "    - Compiles the one or more JSONL data files for a dataset, and constructs a `tf.data.Dataset()` generator for data preprocessing and model feeding.\n",
    "- Model architecture (`get_model()`):\n",
    "    - Builds the corresponding model architecture.\n",
    "- Training (`train_model()`):\n",
    "    - Trains the model\n",
    "- Model artifact saving\n",
    "    - Saves the model artifacts where the Cloud Storage location is determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_training/train_pytorch.py\n",
    "\n",
    "\"\"\"\n",
    "train_pytorch.py is the module for training a Tensorflow Image classifier pipeline\n",
    "\"\"\"\n",
    "\n",
    "# Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import copy\n",
    "\n",
    "import timm\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "## Read environmental variables\n",
    "TRAINING_DATA_PATH = os.environ[\"AIP_TRAINING_DATA_URI\"].replace(\"gs://\", \"/gcs/\")\n",
    "TEST_DATA_PATH = os.environ[\"AIP_TEST_DATA_URI\"].replace(\"gs://\", \"/gcs/\")\n",
    "MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"].replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Helpers -----------------------------------------------------------------------------------------------------------------------------\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data files arguments\n",
    "    parser.add_argument(\"--train-data-dir\", default=os.getenv('AIP_TRAINING_DATA_URI'), dest=\"train_data_dir\", type=str, help=\"train data directory\")\n",
    "    parser.add_argument(\"--val-data-dir\", default=os.getenv('AIP_VALIDATION_DATA_URI'), dest=\"val_data_dir\", type=str, help=\"validation data directory\")\n",
    "    \n",
    "    # data preprocessing\n",
    "    parser.add_argument(\"--image-width\", dest=\"image_width\", default=384, type=int, help=\"image width\")\n",
    "    parser.add_argument(\"--image-height\", dest=\"image_height\", default=384, type=int, help=\"image height\")\n",
    "    \n",
    "    parser.add_argument(\"--lr\", dest=\"lr\",\n",
    "                        default=0.01, type=float,\n",
    "                        help=\"max_depth value.\")\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", dest=\"batch_size\", default=32, type=int, help=\"mini-batch size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", default=10, type=int, help=\"number of training epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--steps\", dest=\"steps\", default=92, type=int, help=\"Number of steps per epoch.\",\n",
    "    )\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# def get_data():\n",
    "#     class_names = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "#     class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "#     num_classes = len(class_names)\n",
    "\n",
    "#     def parse_image(filename):\n",
    "#         image = tf.io.read_file(filename)\n",
    "#         image = tf.image.decode_jpeg(image, channels=3)\n",
    "#         image = tf.image.resize(image, [args.image_width, args.image_height])\n",
    "#         return image\n",
    "\n",
    "#     def extract(data_dir, batch_size=args.batch_size, repeat=True):\n",
    "#         data = []\n",
    "#         labels = []\n",
    "#         for data_uri in tf.io.gfile.glob(pattern=data_dir):\n",
    "#             with tf.io.gfile.GFile(name=data_uri, mode=\"r\") as gfile:\n",
    "#                 for line in gfile.readlines():\n",
    "#                     instance = json.loads(line)\n",
    "#                     data.append(instance[\"imageGcsUri\"])\n",
    "#                     classification_annotation = instance[\"classificationAnnotations\"][0]\n",
    "#                     label = classification_annotation[\"displayName\"]\n",
    "#                     labels.append(class_indices[label])\n",
    "\n",
    "#         data_dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "#         data_dataset = data_dataset.map(\n",
    "#             parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "#         )\n",
    "\n",
    "#         label_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
    "\n",
    "#         dataset = tf.data.Dataset.zip((data_dataset, label_dataset)).cache().shuffle(batch_size * 32)\n",
    "#         if repeat:\n",
    "#             dataset = dataset.repeat()\n",
    "#         dataset = dataset.batch(batch_size)\n",
    "\n",
    "#         # Add property to retain the class names\n",
    "#         dataset.class_names = class_names\n",
    "\n",
    "#         return dataset\n",
    "    \n",
    "#     logging.info('Prepare training data')\n",
    "#     train_dataset = extract(args.train_data_dir)\n",
    "\n",
    "#     logging.info('Prepare validation data')\n",
    "#     val_dataset = extract(args.val_data_dir, batch_size=1, repeat=False)\n",
    "\n",
    "#     return num_classes, train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def get_model(num_classes):\n",
    "    logging.info(\"Get model architecture\")\n",
    "    \n",
    "    # model_ft = models.efficientnet_v2_s(weights=torchvision.models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "    \n",
    "    model_ft = timm.create_model('regnetx_040', num_classes=num_classes, pretrained=True) \n",
    "    \n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    if hasattr(model_ft, 'head'):\n",
    "        for param in model_ft.head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(model_ft, 'fc'):\n",
    "        for param in model_ft.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(model_ft, 'classifier'):\n",
    "        for param in model_ft.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    return model_ft\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "        \n",
    "# num_classes, train_dataset, val_dataset = get_data()\n",
    "\n",
    "\n",
    "model_name = \"efficientnet_v2_s\"\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "feature_extract = True\n",
    "input_size=224\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.RandomResizedCrop(input_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "ds = torchvision.datasets.ImageFolder(\n",
    "    args.train_data_dir.replace(\"gs://\", \"/gcs/\"), \n",
    "    data_transforms\n",
    ")\n",
    "\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [round(len(ds)*0.8),round(len(ds)*0.2)])\n",
    "image_datasets = {'train': train_ds, 'val': val_ds}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "model = get_model(num_classes=num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model, hist = train_model(model, dataloaders_dict, criterion, optimizer_ft, num_epochs=args.epochs, is_inception=(model_name==\"inception\"))\n",
    "\n",
    "\n",
    "def makedirs(model_dir):\n",
    "    if os.path.exists(model_dir) and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "    return\n",
    "\n",
    "makedirs(MODEL_DIR)\n",
    "model_path = os.path.join(MODEL_DIR, \"model.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Image models with serving functions\n",
    "\n",
    "Previously, your model server took input as a 3-dimensional array. Image models typically take a compressed image and use a serving function fused to the model to decompress the compressed image into a 3-dimensional array, and other preprocesing -- such as normalizing the pixel values.\n",
    "\n",
    "Next, you upload your custom image model as a `Vertex AI Model` resource with a serving function. During upload, you define a serving function to convert data to the format your model expects. If you send encoded data to Vertex AI, your serving function ensures that the data is decoded on the model server before it is passed as input to your model.\n",
    "\n",
    "##### How does the serving function work\n",
    "\n",
    "When you send a request to an online prediction server, the request is received by a HTTP server. The HTTP server extracts the prediction request from the HTTP request content body. The extracted prediction request is forwarded to the serving function. For Google pre-built prediction containers, the request content is passed to the serving function as a `tf.string`.\n",
    "\n",
    "The serving function consists of two parts:\n",
    "\n",
    "- `preprocessing function`:\n",
    "  - Converts the input (`tf.string`) to the input shape and data type of the underlying model (dynamic graph).\n",
    "  - Performs the same preprocessing of the data that was done during training the underlying model -- e.g., normalizing, scaling, etc.\n",
    "- `post-processing function`:\n",
    "  - Converts the model output to format expected by the receiving application -- e.q., compresses the output.\n",
    "  - Packages the output for the the receiving application -- e.g., add headings, make JSON object, etc.\n",
    "\n",
    "Both the preprocessing and post-processing functions are converted to static graphs which are fused to the model. The output from the underlying model is passed to the post-processing function. The post-processing function passes the converted/packaged output back to the HTTP server. The HTTP server returns the output as the HTTP response content.\n",
    "\n",
    "One consideration you need to consider when building serving functions for TF.Keras models is that they run as static graphs. That means, you cannot use TF graph operations that require a dynamic graph. If you do, you will get an error during the compile of the serving function which will indicate that you are using an EagerTensor which is not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a custom image for tensorflow model training\n",
    "\n",
    "Here we will build a custom container. A custom container is a Docker image that you create to run your training application. By running your machine learning (ML) training job in a custom container, you can use ML frameworks, non-ML dependencies, libraries, and binaries that are not otherwise supported on Vertex AI. In othere word, we package training code on our local machine into a Docker container image, push the container image to Container Registry, and create a CustomJob.\n",
    "\n",
    "For the ML framework we will use tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image repo\n",
    "!gcloud artifacts repositories create $IMAGE_REPOSITORY \\\n",
    "    --repository-format=docker \\\n",
    "    --location=europe-west4 \\\n",
    "    --description=\"Vision Workshop Docker Image repository\"\n",
    "\n",
    "# List repositories under the project\n",
    "!gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker europe-west4-docker.pkg.dev -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_training/Dockerfile\n",
    "# Specifies base image and tag\n",
    "FROM pytorch/pytorch:1.11.0-cuda11.3-cudnn8-runtime\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip install timm\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY ./train_pytorch.py /root/train_pytorch.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"train_pytorch.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and push docker file\n",
    "!docker build -t $IMAGE_URI ./build_training/\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the script to run on Vertex AI\n",
    "In this section, we create a training pipeline. It will create custom training jobs, load our dataset and upload the model to Vertex AI after the training job is successfully completed. Learn more about creating of custom jobs [here](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME, experiment=\"train\")\n",
    "\n",
    "\n",
    "ds = vertex_ai.ImageDataset.list(filter=\"display_name=flowers\")[0]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertex_ai.CustomContainerTrainingJob(\n",
    "    display_name=TRAIN_JOB_NAME,\n",
    "    container_uri=IMAGE_URI,\n",
    ")\n",
    "\n",
    "parameters = {\"LR\": 0.003}\n",
    "\n",
    "CMDARGS = [ f\"\"\"--train-data-dir=gs://{BUCKET_NAME}/flowers\"\"\",\n",
    "    \"--lr=\" + str(parameters[\"LR\"]),\n",
    "    \"--epochs=10\"\n",
    "]\n",
    "\n",
    "\n",
    "_ = job.run(\n",
    "    dataset=ds,\n",
    "    annotation_schema_uri=vertex_ai.schema.dataset.annotation.image.classification,\n",
    "    # model_display_name=MODEL_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\"LR\": 0.003}\n",
    "\n",
    "# CMDARGS = [ f\"\"\"--train-data-dir=gs://{BUCKET_NAME}/flowers\"\"\",\n",
    "#     \"--lr=\" + str(parameters[\"LR\"]),\n",
    "#     \"--epochs=10\"\n",
    "# ]\n",
    "\n",
    "# job = vertex_ai.CustomTrainingJob(\n",
    "#     display_name=TRAIN_JOB_NAME,\n",
    "#     script_path=\"build_training/train_tf.py\",\n",
    "#     container_uri=\"europe-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-11:latest\",\n",
    "#     requirements=[\"gcsfs\", \"timm\"],\n",
    "#     # model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    "# )\n",
    "\n",
    "\n",
    "# model = job.run(\n",
    "#     dataset=ds,\n",
    "#     annotation_schema_uri=vertex_ai.schema.dataset.annotation.image.classification,\n",
    "#     # model_display_name=MODEL_NAME,\n",
    "#     args=CMDARGS,\n",
    "#     replica_count=1,\n",
    "#     machine_type=\"n1-standard-4\",\n",
    "#     accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "#     accelerator_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Deploy the model\n",
    "Before you use your model to make predictions, you need to deploy it to an Endpoint. You can do this by calling the deploy function on the Model resource. This will do two things:\n",
    "\n",
    "- create an Endpoint resource\n",
    "- deploy the Model resource to the Endpoint resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.gca_resource.training_task_inputs['baseOutputDirectory']['outputUriPrefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_TRAINED_MODEL_URI = f\"{job.gca_resource.training_task_inputs['baseOutputDirectory']['outputUriPrefix']}/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $GCS_TRAINED_MODEL_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir trained_model\n",
    "!gsutil -m cp -r $GCS_TRAINED_MODEL_URI/* ./trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_DIRECTORY = \"./predictor\"\n",
    "\n",
    "!mkdir $PREDICTOR_DIRECTORY\n",
    "!cp ./trained_model/* ./predictor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $PREDICTOR_DIRECTORY/custom_handler.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "from torchvision import models, transforms\n",
    "import timm\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TransformersClassifierHandler(BaseHandler):\n",
    "    \"\"\"\n",
    "    The handler takes an input string and returns the classification text \n",
    "    based on the serialized transformers checkpoint.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TransformersClassifierHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        \"\"\" Loads the model.pt file and initialized the model object.\n",
    "        Instantiates Tokenizer for preprocessor to use\n",
    "        Loads labels to name mapping file for post-processing inference response\n",
    "        \"\"\"\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.pt or pytorch_model.bin file\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = timm.create_model('regnetx_040', num_classes=5) \n",
    "        self.model.load_state_dict(torch.load(model_pt_path, map_location=self.device))\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        logger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n",
    "\n",
    "        # Read the mapping file, index to object name\n",
    "        mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n",
    "\n",
    "        if os.path.isfile(mapping_file_path):\n",
    "            with open(mapping_file_path) as f:\n",
    "                self.mapping = json.load(f)\n",
    "        else:\n",
    "            logger.warning('Missing the index_to_name.json file. Inference output will default.')\n",
    "            self.mapping = {\"0\": \"Negative\",  \"1\": \"Positive\"}\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        \"\"\" Preprocessing input request by tokenizing\n",
    "            Extend with your own preprocessing steps as needed\n",
    "        \"\"\"\n",
    "        bytes_input = data[0].get(\"data\")\n",
    "        if bytes_input is None:\n",
    "            bytes_input = data[0].get(\"body\")\n",
    "        \n",
    "        img = Image.open(io.BytesIO(bytes_input))\n",
    "\n",
    "        data_transforms = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        img = data_transforms(img)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return torch.unsqueeze(img, 0)\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        \"\"\" Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        print(\"### INFERENCE ###\")\n",
    "        print(inputs)\n",
    "        predictions = self.model(inputs.to(self.device))\n",
    "        print(predictions)\n",
    "        index = predictions[0].argmax().item()\n",
    "        \n",
    "        if self.mapping:\n",
    "            prediction = self.mapping[str(index)]\n",
    "        \n",
    "        percentage = (torch.nn.functional.softmax(predictions[0]) * 100)[index].item()\n",
    "\n",
    "        logger.info(\"Model predicted: '%s', percentage %s\", prediction, percentage)\n",
    "        return [{\"prediction\" : prediction, \"percentage\" : percentage}]\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        return inference_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $PREDICTOR_DIRECTORY/index_to_name.json\n",
    "\n",
    "{\n",
    "    \"0\": \"daisy\", \n",
    "    \"1\": \"dandelion\", \n",
    "    \"2\": \"roses\", \n",
    "    \"3\": \"sunflowers\", \n",
    "    \"4\": \"tulips\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $PREDICTOR_DIRECTORY/config.properties\n",
    "service_envelope=json\n",
    "inference_address=http://0.0.0.0:7080\n",
    "management_address=http://0.0.0.0:7081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $PREDICTOR_DIRECTORY/Dockerfile\n",
    "\n",
    "FROM pytorch/torchserve:latest-cpu\n",
    "\n",
    "RUN pip3 install timm\n",
    "\n",
    "# copy model artifacts, custom handler and other dependencies\n",
    "COPY . /home/model-server/\n",
    "WORKDIR /home/model-server/\n",
    "\n",
    "USER model-server\n",
    "\n",
    "EXPOSE 7080\n",
    "EXPOSE 7081\n",
    "\n",
    "# create model archive file packaging model artifacts and dependencies\n",
    "RUN torch-model-archiver -f \\\n",
    "  --model-name=model \\\n",
    "  --version=1.0 \\\n",
    "  --serialized-file=/home/model-server/model.pth \\\n",
    "  --handler=/home/model-server/custom_handler.py \\\n",
    "  --extra-files \"/home/model-server/index_to_name.json\" \\\n",
    "  --export-path=/home/model-server/model-store\n",
    "\n",
    "# run Torchserve HTTP serve to respond to prediction requests\n",
    "CMD [\"torchserve\", \\\n",
    "     \"--start\", \\\n",
    "     \"--ts-config=/home/model-server/config.properties\", \\\n",
    "     \"--models\", \\\n",
    "     \"model=model.mar\", \\\n",
    "     \"--model-store\", \\\n",
    "     \"/home/model-server/model-store\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"europe-west4-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}-predictor:{IMAGE_TAG}\"\n",
    "!docker build --tag={CUSTOM_PREDICTOR_IMAGE_URI} ./predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run docker container to start local TorchServe deployment\n",
    "!docker run -t -d --rm -p 7080:7080 --name=local_bert_classifier $CUSTOM_PREDICTOR_IMAGE_URI\n",
    "# delay to allow the model to be loaded in torchserve (takes a few seconds)\n",
    "!sleep 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import PIL\n",
    "from torchvision import transforms\n",
    "import base64\n",
    "\n",
    "\n",
    "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
    "sunflower_path = \"592px-Red_sunflower.jpg\"\n",
    "urllib.request.urlretrieve(sunflower_url, sunflower_path)\n",
    "\n",
    "img = PIL.Image.open(sunflower_path).resize((224, 224))\n",
    "from io import BytesIO\n",
    "\n",
    "buffered = BytesIO()\n",
    "img.save(buffered, format=\"JPEG\")\n",
    "\n",
    "bytes_input = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash  -s \"$bytes_input\"\n",
    "\n",
    "cat > ./predictor/instances.json <<END\n",
    "{\n",
    "   \"instances\": [\n",
    "     {\n",
    "       \"data\": {\n",
    "         \"b64\": \"$1\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./predictor/instances.json \\\n",
    "  http://localhost:7080/predictions/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop local_bert_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCHIVED_MODEL_PATH = \"./archived_model\"\n",
    "\n",
    "# !mkdir $ARCHIVED_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# # Google Cloud Notebook requires to add a path to find the installed torch-model-archiver\n",
    "# if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "#     os.environ[\"PATH\"] = f'{os.environ.get(\"PATH\")}:~/.local/bin'\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch-model-archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !torch-model-archiver -f \\\n",
    "#   --model-name=model \\\n",
    "#   --version=1.0 \\\n",
    "#   --serialized-file=trained_model/model.pth \\\n",
    "#   --handler=$PREDICTOR_DIRECTORY/custom_handler.py \\\n",
    "#   --extra-files \"$PREDICTOR_DIRECTORY/index_to_name.json\" \\\n",
    "#   --export-path=$ARCHIVED_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCHIVED_MODEL_GCS_URI = f\"gs://{BUCKET_NAME}/archived-pytorch-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil cp -r $ARCHIVED_MODEL_PATH $ARCHIVED_MODEL_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil ls -al $ARCHIVED_MODEL_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = f\"pytorch-v{VERSION}\"\n",
    "model_description = \"PyTorch based image classifier with the pre-built PyTorch image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serving_container_image_uri = \"europe-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-11:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vertex_ai.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "    serving_container_ports=[7080],\n",
    "    serving_container_predict_route=f\"/predictions/model\",\n",
    "    serving_container_health_route=\"/ping\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vertex_ai.Model('projects/446303513828/locations/europe-west4/models/937074177934884864@1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XAI = \"ig\"  # [ shapley, ig, xrai ]\n",
    "\n",
    "# if XAI == \"shapley\":\n",
    "#     PARAMETERS = {\"sampled_shapley_attribution\": {\"path_count\": 10}}\n",
    "# elif XAI == \"ig\":\n",
    "#     PARAMETERS = {\"integrated_gradients_attribution\": {\"step_count\": 50}}\n",
    "# elif XAI == \"xrai\":\n",
    "#     PARAMETERS = {\"xrai_attribution\": {\"step_count\": 50}}\n",
    "\n",
    "# parameters = vertex_ai.explain.ExplanationParameters(PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_metadata = vertex_ai.explain.ExplanationMetadata.InputMetadata({\"input_tensor_name\": \"numpy_inputs\", \"modality\": \"image\"})\n",
    "# output_metadata = vertex_ai.explain.ExplanationMetadata.OutputMetadata({\"output_tensor_name\": \"output_0\"})\n",
    "\n",
    "# metadata = vertex_ai.explain.ExplanationMetadata(\n",
    "#     inputs={\"image\": input_metadata}, outputs={\"class\": output_metadata}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_COMPUTE=\"n1-standard-4\"\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    accelerator_count=0,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    # explanation_parameters=parameters,\n",
    "    # explanation_metadata=metadata,\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import PIL\n",
    "from torchvision import transforms\n",
    "import base64\n",
    "\n",
    "\n",
    "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
    "sunflower_path = \"592px-Red_sunflower.jpg\"\n",
    "urllib.request.urlretrieve(sunflower_url, sunflower_path)\n",
    "\n",
    "img = PIL.Image.open(sunflower_path).resize((224, 224))\n",
    "from io import BytesIO\n",
    "\n",
    "buffered = BytesIO()\n",
    "img.save(buffered, format=\"JPEG\")\n",
    "\n",
    "bytes_input = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [{'data': {'b64': bytes_input}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.predict(instances=instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "\n",
    "# for explanation in endpoint.explain(instances=instances).explanations:\n",
    "#     attributions = dict(explanation.attributions[0].feature_attributions)\n",
    "#     label_index = explanation.attributions[0].output_index[0]\n",
    "#     class_name = class_names[label_index]\n",
    "#     b64str = attributions[\"image\"][\"b64_jpeg\"]\n",
    "#     image = base64.b64decode(b64str)\n",
    "#     image = io.BytesIO(image)\n",
    "#     img = mpimg.imread(image, format=\"JPG\")\n",
    "\n",
    "#     plt.imshow(img, interpolation=\"nearest\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the deployed model (Make an online prediction request)\n",
    "Send an online prediction request to your deployed model. To make sure your deployed model is working, test it out by sending a request to the endpoint.\n",
    "\n",
    "Let's first get a test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import base64\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = vertex_ai.Endpoint.list(filter=f\"display_name={MODEL_NAME}_endpoint\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vertex_ai.Model.list(filter=f\"display_name={MODEL_NAME}\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
    "sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n",
    "\n",
    "\n",
    "img = Image.open(sunflower_path).resize((384, 384))\n",
    "from io import BytesIO\n",
    "\n",
    "buffered = BytesIO()\n",
    "img.save(buffered, format=\"JPEG\")\n",
    "\n",
    "bytes_input = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "instances = [{'data': {'b64': bytes_input}}]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "predictions = endpoint.predict(instances=instances)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch predictions\n",
    "\n",
    "Send a batch prediction request to your deployed model.\n",
    "\n",
    "Batch prediction provides the ability to do offline batch processing of large amounts of prediction requests. Resources are only provisioned during the batch process and then deprovisioned when the batch request is completed. The results are stored in Cloud Storage, in contrast to online prediction where the results are returned as a HTTP response packet.\n",
    "\n",
    "The input format for your batch job is dependent on the format supported by your model server. Foremost, the web server in your model server must support a JSONL format, which the web server will convert to a format support either directly by the model input intertace or a serving function interface. For batch prediction, this JSONL format is referred to as the `pivot` format.\n",
    "\n",
    "### Input format for batch prediction jobs\n",
    "\n",
    "The batch server accepts the following input formats for custom image models:\n",
    "\n",
    "- JSONL\n",
    "- File-List\n",
    "\n",
    "### Output format for batch prediction jobs\n",
    "\n",
    "The batch server accepts the following output formats for custom image models:\n",
    "\n",
    "- JSONL\n",
    "\n",
    "### Pivot format\n",
    "\n",
    "The batch server converts the input format to the `pivot` (JSONL) format as follows:\n",
    "\n",
    "**JSONL**\n",
    "\n",
    "Each input line (request) should contain one and only one valid json value.\n",
    "\n",
    "    {\"values\": [1, 2, 3, 4], \"key\": 1}\n",
    "    {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
    "\n",
    "The batch server generates the pivot data with the same format. The generated pivot data is then wrapped into a payload request:\n",
    "\n",
    "    {\"instances\": [\n",
    "      {\"values\": [1, 2, 3, 4], \"key\": 1},\n",
    "      {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
    "    ]}\n",
    "\n",
    "**FileList**\n",
    "\n",
    "The FileList format contains a list of files. Each line in a “FileList” file specifies a single file path, specified as a Cloud Storage location.\n",
    "\n",
    "    gs://my-bucket/file1.txt\n",
    "    gs://my-bucket/file2.txt\n",
    "\n",
    "The batch server reads the files as binaries. The binary objects are serialized as ASCII strings.\n",
    "\n",
    "    {\"instances\": [\n",
    "     {\"b64\",\"b64EncodedASCIIString\"},\n",
    "     {\"b64\",\"b64EncodedASCIIString\"}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create such a `FileList` input file by sampling ten images per label from our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client() \n",
    "\n",
    "daisies = list(client.list_blobs(BUCKET_NAME, prefix='flowers/daisy'))[:10]\n",
    "tullips = list(client.list_blobs(BUCKET_NAME, prefix='flowers/tulips'))[:10]\n",
    "roses = list(client.list_blobs(BUCKET_NAME, prefix='flowers/roses'))[:10]\n",
    "sunflowers = list(client.list_blobs(BUCKET_NAME, prefix='flowers/sunflowers'))[:10]\n",
    "dandelions = list(client.list_blobs(BUCKET_NAME, prefix='flowers/dandelion'))[:10]\n",
    "\n",
    "images = daisies + tullips + roses + sunflowers + dandelions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(client.list_blobs(BUCKET_NAME, prefix='flowers/dandelion'))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [f\"gs://{image.bucket.name}/{image.name}\" for image in images]\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from io import BytesIO\n",
    "\n",
    "gcs_input_uri = f\"gs://{BUCKET_NAME}/flowers_batch.txt\"\n",
    "\n",
    "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
    "    for image in images:\n",
    "        f.write(image + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send the prediction request\n",
    "\n",
    "To make a batch prediction request, call the model object's `batch_predict` method with the following parameters: \n",
    "- `instances_format`: The format of the batch prediction request file: \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" or \"file-list\"\n",
    "- `prediction_format`: The format of the batch prediction response file: \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" or \"file-list\"\n",
    "- `job_display_name`: The human readable name for the prediction job.\n",
    " - `gcs_source`: A list of one or more Cloud Storage paths to your batch prediction requests.\n",
    "- `gcs_destination_prefix`: The Cloud Storage path that the service will write the predictions to.\n",
    "- `model_parameters`: Additional filtering parameters for serving prediction results.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n",
    "\n",
    "### Compute instance scaling\n",
    "\n",
    "You can specify a single instance (or node) to process your batch prediction request. This tutorial uses a single node, so the variables `MIN_NODES` and `MAX_NODES` are both set to `1`.\n",
    "\n",
    "If you want to use multiple nodes to process your batch prediction request, set `MAX_NODES` to the maximum number of nodes you want to use. Vertex AI autoscales the number of nodes used to serve your predictions, up to the maximum number you set. Refer to the [pricing page](https://cloud.google.com/vertex-ai/pricing#prediction-prices) to understand the costs of autoscaling with multiple nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=f\"{DEPLOYED_NAME}_batch\",\n",
    "    gcs_source=gcs_input_uri,\n",
    "    gcs_destination_prefix=f\"gs://{BUCKET_NAME}\",\n",
    "    instances_format=\"file-list\",\n",
    "    model_parameters=None,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    starting_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    generate_explanation=False,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "print(batch_predict_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the predictions\n",
    "\n",
    "Next, get the results from the completed batch prediction job.\n",
    "\n",
    "The results are written to the Cloud Storage output bucket you specified in the batch prediction request. You call the method iter_outputs() to get a list of each Cloud Storage file generated with the results. Each file contains one or more prediction requests in a JSON format:\n",
    "\n",
    "- `instance`: The prediction request.\n",
    "- `prediction`: The prediction response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_job.output_info.gcs_output_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "filenames = [f\"gs://{blob.bucket.name}/{blob.name}\" for blob in bp_iter_outputs if blob.name.split(\"/\")[-1].startswith(\"prediction.results\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [pd.read_json(filename, lines=True) for filename in filenames]\n",
    "batch_predictions = pd.concat(dataframes)\n",
    "batch_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "batch_predictions.prediction = batch_predictions.prediction.apply(lambda x: tf.nn.softmax(x))\n",
    "batch_predictions.prediction = batch_predictions.prediction.apply(lambda score: \"{} {:.2f}%\".format(class_names[np.argmax(score)], 100 * np.max(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
