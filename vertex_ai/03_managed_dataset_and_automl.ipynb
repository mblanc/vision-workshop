{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7376c-ae1c-4640-a33a-a14717dbf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba01c5d-1c80-4f9a-9403-3e1e8ddc7ef3",
   "metadata": {},
   "source": [
    "# Vision Workshop - Managed Dataset and AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610f30a-095a-48fa-bae0-ed2f14277752",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "[Vision Workshop](https://github.com/mblanc/vision-workshop) is a series of labs on how to build an image classification system on Google Cloud. Throughout the Vision Workshop labs, you will learn how to read image data stored in data lake, perform exploratory data analysis (EDA), train a model, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd592c2-41db-4897-8e5d-9999d9a43fbc",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to pull features from Feature Store for training, run data exploratory analysis on features, build a machine learning model locally, experiment with various hyperparameters, evaluate the model and deloy it to a Vertex AI endpoint. \n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "- Use a Feature Store to pull training data\n",
    "- Do some exploratory analysis on the extracted data\n",
    "- Train the model and track the results using Vertex AI Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c6685-6ed5-497f-b170-9b112694f1f7",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfaa8b8-14fd-4912-9af9-44260d072d4f",
   "metadata": {},
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03cf7c7-0741-49b0-a8f7-4b58c4376ce1",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9206cf-4669-4fe1-8708-879db35a05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-vision-workshop\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4249900-b8a5-47b5-bb7d-fc74df1ce6b3",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f8ef9-aa81-4ec6-9b77-5e2c7e9eadd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c13d9-cc61-4702-8d2e-3069888736e2",
   "metadata": {},
   "source": [
    "### Vertex AI Managed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a894670-8dbe-435d-bf93-0b0b742b0bf9",
   "metadata": {},
   "source": [
    "To load an image dataset in Vertex AI Managed Dataset, you will need to create a file listing your images and their label(s).\n",
    "\n",
    "see [Prepare image training data for classification](https://cloud.google.com/vertex-ai/docs/image-data/classification/prepare-data)\n",
    "\n",
    "This input file can be in the `CSV` or `JSONL` format.\n",
    "\n",
    "For the `CSV` format :\n",
    "\n",
    "CSV format:\n",
    "\n",
    "\n",
    "```[ML_USE],GCS_FILE_PATH,[LABEL]```\n",
    "\n",
    "List of columns\n",
    "\n",
    "* `ML_USE` (Optional) - For data split purposes when training a model. Use TRAINING, TEST, or VALIDATION. For more information about manual data splitting, see About data splits for AutoML models.\n",
    "* `GCS_FILE_PATH` - This field contains the Cloud Storage URI for the image. Cloud Storage URIs are case-sensitive.\n",
    "* `LABEL` (Optional) - Labels must start with a letter and only contain letters, numbers, and underscores.\n",
    "\n",
    "Example CSV - image_classification_single_label.csv:\n",
    "\n",
    "```\n",
    "test,gs://bucket/filename1.jpeg,daisy\n",
    "training,gs://bucket/filename2.gif,dandelion\n",
    "gs://bucket/filename3.png\n",
    "gs://bucket/filename4.bmp,sunflowers\n",
    "validation,gs://bucket/filename5.tiff,tulips\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ddd06-3089-4c91-ad3a-a247042ab6a8",
   "metadata": {},
   "source": [
    "Let's create such an input file from our image dataset and store it on GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc14ef-5290-4fca-93fa-088609d8a890",
   "metadata": {},
   "source": [
    "We list all the images in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ef810-b83f-45d5-8549-51300ae73a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client() \n",
    "\n",
    "blobs = list(client.list_blobs(BUCKET_NAME, prefix='flowers/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2fa692-1903-45ee-b6e6-65a4cd33caed",
   "metadata": {},
   "source": [
    "We extract their uris and their label from the name of the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c5c59-8a60-46ce-b86c-1166a8ab9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [[f\"gs://{blob.bucket.name}/{blob.name}\", os.path.split(os.path.dirname(blob.name))[1]] for blob in blobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba6abf-1b5f-4d0e-94be-bc3de0d2504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e078a6-d768-4642-a824-1f9de51e7a9b",
   "metadata": {},
   "source": [
    "We save the result as a `CSV` file directly on our GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db194087-1da6-4f72-9b6f-c4364dfa6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"gs://{BUCKET_NAME}/flowers/flowers.csv\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65663fa6-23c0-4e43-83be-958ea9b517d0",
   "metadata": {},
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Next, create the Dataset resource using the create method for the ImageDataset class, which takes the following parameters:\n",
    "\n",
    "* `display_name`: The human readable name for the Dataset resource.\n",
    "* `gcs_source`: A list of one or more dataset index files to import the data items into the Dataset resource.\n",
    "* `import_schema_uri`: The data labeling schema for the data items.\n",
    "\n",
    "\n",
    "Learn more about [ImageDataset](https://cloud.google.com/vertex-ai/docs/datasets/prepare-image).\n",
    "\n",
    "This operation may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5958554-fc39-4a23-a031-0eaa97c58cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1de0d-504f-4057-ac5a-9f2a2c39e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = vertex_ai.ImageDataset.create(\n",
    "    display_name=\"flowers\",\n",
    "    gcs_source=f\"gs://{BUCKET_NAME}/flowers/flowers.csv\",\n",
    "    import_schema_uri=vertex_ai.schema.dataset.ioformat.image.single_label_classification,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "ds.wait()\n",
    "\n",
    "print(ds.display_name)\n",
    "print(ds.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fa1d1-1cc5-4891-8771-289698e855f7",
   "metadata": {},
   "source": [
    "### Train an AutoML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8454b2-50a9-4db1-b2a3-c21b2c867d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = vertex_ai.ImageDataset.list(filter=\"display_name=flowers\")[0]\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59f4b5-801d-4fe2-9060-9a58d5849776",
   "metadata": {},
   "source": [
    "To train an AutoML model, you perform two steps: 1) create a training pipeline, and 2) run the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32549d9-6951-49c0-b351-7908bc15369a",
   "metadata": {},
   "source": [
    "#### Create training pipeline\n",
    "An AutoML training pipeline is created with the AutoMLImageTrainingJob class, with the following parameters:\n",
    "\n",
    "* `display_name`: The human readable name for the TrainingJob resource.\n",
    "* `prediction_type`: The type task to train the model for.\n",
    "    * `classification`: An image classification model.\n",
    "    * `object_detection`: An image object detection model.\n",
    "    * `multi_label`: If a classification task, whether single (False) or multi-labeled (True).\n",
    "* `model_type`: The type of model for deployment.\n",
    "    * `CLOUD`: Deployment on Google Cloud\n",
    "    * `CLOUD_HIGH_ACCURACY_1`: Optimized for accuracy over latency for deployment on Google Cloud.\n",
    "    * `CLOUD_LOW_LATENCY_`: Optimized for latency over accuracy for deployment on Google Cloud.\n",
    "    * `MOBILE_TF_VERSATILE_1`: Deployment on an edge device.\n",
    "    * `MOBILE_TF_HIGH_ACCURACY_1`:Optimized for accuracy over latency for deployment on an edge device.\n",
    "    * `MOBILE_TF_LOW_LATENCY_1`: Optimized for latency over accuracy for deployment on an edge device.\n",
    "* `base_model`: (optional) Transfer learning from existing Model resource -- supported for image classification only.\n",
    "\n",
    "The instantiated object is the DAG (directed acyclic graph) for the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725d112-a239-4622-b1f8-4fc7c9113699",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertex_ai.AutoMLImageTrainingJob(\n",
    "    display_name=\"flowers_automl_job\",\n",
    "    prediction_type=\"classification\",\n",
    "    model_type=\"CLOUD\",\n",
    "    multi_label=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369670c-3cab-4960-a652-4a4f9bc02302",
   "metadata": {},
   "source": [
    "#### Run the training pipeline\n",
    "\n",
    "Next, you run the DAG to start the training job by invoking the method run, with the following parameters:\n",
    "\n",
    "* `dataset`: The Dataset resource to train the model.\n",
    "* `model_display_name`: The human readable name for the trained model.\n",
    "* `training_fraction_split`: The percentage of the dataset to use for training.\n",
    "* `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
    "* `validation_fraction_split`: The percentage of the dataset to use for validation.\n",
    "* `budget_milli_node_hours`: (optional) Maximum training time specified in unit of millihours (1000 = hour).\n",
    "* `disable_early_stopping`: If True, training maybe completed before using the entire budget if the service believes it cannot further improve on the model objective measurements.\n",
    "\n",
    "The `run` method when completed returns the `Model` resource.\n",
    "\n",
    "The execution of the training pipeline will take upto 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906764e5-33ca-40ad-bbf9-cc98fc0af9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = job.run(\n",
    "    dataset=ds,\n",
    "    model_display_name=\"flowers_automl\",\n",
    "    budget_milli_node_hours=8000,\n",
    "    training_fraction_split=0.8,\n",
    "    validation_fraction_split=0.1,\n",
    "    test_fraction_split=0.1,\n",
    "    disable_early_stopping=False,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d72b5b-d2a5-4eb1-a237-7fee94d67345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
