{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vision Workshop - ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Vision Workshop](https://github.com/mblanc/vision-workshop) is a series of labs on how to build an image classification system on Google Cloud. Throughout the Vision Workshop labs, you will learn how to read image data stored in data lake, perform exploratory data analysis (EDA), train a model, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to pull features from Feature Store for training, run data exploratory analysis on features, build a machine learning model locally, experiment with various hyperparameters, evaluate the model and deloy it to a Vertex AI endpoint. \n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "- Create and deploy a Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-vision-workshop\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf",
    "tags": []
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components import aiplatform as vertex_ai_components\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "import tensorflow as tf\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.components import importer_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kfp version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google_cloud_pipeline_components\n",
    "print(\"tf version:\", tf.__version__)\n",
    "print(\"kfp version:\", kfp.__version__)\n",
    "print(\"google-cloud-pipeline-components version:\", google_cloud_pipeline_components.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "PIPELINE_NAME = f'vision-workshop-tf-pipeline-{ID}'\n",
    "PIPELINE_DIR=os.path.join(os.curdir, 'pipelines')\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipelines\"\n",
    "PIPELINE_PACKAGE_PATH = f\"{PIPELINE_DIR}/pipeline_{ID}.json\"\n",
    "COMPONENTS_DIR=os.path.join(os.curdir, 'pipelines', 'components')\n",
    "\n",
    "# #Dataset component\n",
    "DATASET_NAME = f'flowers'\n",
    "\n",
    "# #Training component\n",
    "JOB_NAME = f'image-classifier-train-tf-{ID}'\n",
    "MODEL_NAME = f'image-classifier-tf-{ID}'\n",
    "TRAIN_MACHINE_TYPE = 'n1-standard-4'\n",
    "MODEL_SERVING_IMAGE_URI = \"europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    "ARGS=[ \"--lr=0.003\", \"--epochs=10\"]\n",
    "IMAGE_REPOSITORY = f\"vision-{ID}\"\n",
    "IMAGE_NAME=\"image-classifier\"\n",
    "IMAGE_TAG=\"v1\"\n",
    "IMAGE_URI=f\"europe-west4-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertex AI client\n",
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p -m 777 $PIPELINE_DIR $COMPONENTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline using ```kfp``` and ```google_cloud_pipeline_components```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = vertex_ai.ImageDataset.list(filter=\"display_name=flowers\", location=REGION)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=PIPELINE_NAME,\n",
    ")\n",
    "def pipeline(project_id:str = PROJECT_ID, \n",
    "             region:str = REGION, \n",
    "             bucket_name:str = f\"gs://{BUCKET_NAME}\",\n",
    "             replica_count:int = 1,\n",
    "             machine_type:str = \"n1-standard-4\",\n",
    "            ):\n",
    "    \n",
    "    #create dataset \n",
    "    # dataset_create_op = vertex_ai_components.ImageDatasetCreateOp(project=project_id,\n",
    "    #                                                    location=region,\n",
    "    #                                                    display_name=DATASET_NAME,\n",
    "    #                                                    import_schema_uri=vertex_ai.schema.dataset.ioformat.image.single_label_classification,\n",
    "    #                                                    gcs_source=f\"gs://{BUCKET_NAME}/prod/flowers.csv\")\n",
    "    \n",
    "    importer_op = importer_node.importer(\n",
    "        artifact_uri=f\"https://{ds.location}-aiplatform.googleapis.com/v1/{ds.resource_name}\",\n",
    "        artifact_class=artifact_types.VertexDataset,\n",
    "        metadata={\n",
    "            \"resourceName\": ds.resource_name,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    #custom training job component - script\n",
    "    train_model_op = vertex_ai_components.CustomContainerTrainingJobRunOp(\n",
    "        display_name=JOB_NAME,\n",
    "        model_display_name=MODEL_NAME,\n",
    "        container_uri=IMAGE_URI,\n",
    "        staging_bucket=bucket_name,\n",
    "        dataset= importer_op.output, #dataset_create_op.outputs['dataset'],\n",
    "        annotation_schema_uri=vertex_ai.schema.dataset.annotation.image.classification,\n",
    "        base_output_dir=bucket_name,\n",
    "        args = ARGS,\n",
    "        replica_count= replica_count,\n",
    "        machine_type= machine_type,\n",
    "        accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "        accelerator_count=1,\n",
    "        model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    "        project=project_id,\n",
    "        location=region).after(importer_op)\n",
    "    \n",
    "    # batch_op = ModelBatchPredictOp(\n",
    "    #     project=project_id,\n",
    "    #     location=region,\n",
    "    #     job_display_name=\"batch_predict_job\",\n",
    "    #     model=train_model_op.outputs[\"model\"],\n",
    "    #     gcs_source_uris=[f\"gs://{BUCKET_NAME}/flowers_batch.txt\"],\n",
    "    #     gcs_destination_output_uri_prefix=f\"gs://{BUCKET_NAME}\",\n",
    "    #     instances_format=\"file-list\",\n",
    "    #     predictions_format=\"jsonl\",\n",
    "    #     model_parameters={},\n",
    "    #     machine_type=machine_type,\n",
    "    #     starting_replica_count=1,\n",
    "    #     max_replica_count=1,\n",
    "    # )\n",
    "\n",
    "    \n",
    "    #create endpoint\n",
    "    create_endpoint_op = vertex_ai_components.EndpointCreateOp(\n",
    "        display_name=ENDPOINT_NAME,\n",
    "        project=project_id, \n",
    "        location=region).after(train_model_op)\n",
    "\n",
    "    #deploy the model\n",
    "    custom_model_deploy_op = vertex_ai_components.ModelDeployOp(\n",
    "        model=train_model_op.outputs[\"model\"],\n",
    "        endpoint=create_endpoint_op.outputs[\"endpoint\"],\n",
    "        deployed_model_display_name=MODEL_NAME,\n",
    "        dedicated_resources_machine_type=machine_type,\n",
    "        dedicated_resources_min_replica_count=replica_count\n",
    "    ).after(create_endpoint_op)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_compiler = compiler.Compiler()\n",
    "pipeline_compiler.compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate pipeline representation\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    location=REGION,\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job.run(sync=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
