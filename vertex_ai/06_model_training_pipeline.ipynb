{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Fraudfinder - ML Pipeline\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/fraudfinder/raw/main/06_model_training_pipeline.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/fraudfinder/blob/main/06_model_training_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/fraudfinder/blob/main/06_model_training_pipeline.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to use Feature Store, Pipelines and Model Monitoring for building an end-to-end demo using both components defined in `google_cloud_pipeline_components` and custom components. \n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "- [BigQuery](https://cloud.google.com/bigquery/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "    * Create a Feature Store for store and sharing features\n",
    "    * Create a Pipeline to deploy the model\n",
    "    * Create a Model Monitoring Job to check the status of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUCKET_NAME          = \"temp-vision-workshop-vision-workshop\"\n",
      "PROJECT              = \"temp-vision-workshop\"\n",
      "REGION               = \"europe-west4\"\n",
      "ID                   = \"7l3oe\"\n",
      "MODEL_NAME           = \"vision_workshop_model\"\n",
      "ENDPOINT_NAME        = \"vision_workshop_endpoint\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-vision-workshop\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf",
    "tags": []
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# # Vertex Pipelines\n",
    "# from typing import NamedTuple\n",
    "# import kfp\n",
    "# from kfp.v2 import dsl\n",
    "# from kfp.v2.dsl import Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, Metrics, ClassificationMetrics, Condition, component\n",
    "# from kfp.v2 import compiler\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components import aiplatform as vertex_ai_components\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp\n",
    "# from kfp.v2.google.client import AIPlatformClient as VertexAIClient\n",
    "\n",
    "\n",
    "# import google.cloud.aiplatform as aip\n",
    "# from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "# from kfp.v2 import compiler, dsl\n",
    "# from kfp.v2.dsl import component\n",
    "\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "import tensorflow as tf\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp version: 1.8.14\n"
     ]
    }
   ],
   "source": [
    "print(\"kfp version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Components\n",
    "# # BASE_IMAGE=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\"\n",
    "# BASE_IMAGE='python:3.7'\n",
    "# \n",
    "# INGEST_FEATURE_STORE=f\"{COMPONENTS_DIR}/ingest_feature_store_{ID}.yaml\"\n",
    "# EVALUATE=f\"{COMPONENTS_DIR}/evaluate_{ID}.yaml\"\n",
    "\n",
    "# #Pipeline\n",
    "PIPELINE_NAME = f'vision-workshop-tf-pipeline-{ID}'\n",
    "PIPELINE_DIR=os.path.join(os.curdir, 'pipelines')\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipelines\"\n",
    "PIPELINE_PACKAGE_PATH = f\"{PIPELINE_DIR}/pipeline_{ID}.json\"\n",
    "COMPONENTS_DIR=os.path.join(os.curdir, 'pipelines', 'components')\n",
    "\n",
    "# #Feature Store component\n",
    "# START_DATE_TRAIN = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "# END_DATE_TRAIN = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "# BQ_DATASET = \"tx\"\n",
    "# READ_INSTANCES_TABLE = f\"ground_truth\"\n",
    "# READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "# #Dataset component\n",
    "DATASET_NAME = f'vision_workshop_dataset'\n",
    "# # GCS_SOURCE = [f'{BUCKET_NAME}/data/train/000000000000.csv', f'{BUCKET_NAME}/data/train/000000000001.csv', f'{BUCKET_NAME}/data/train/000000000002.csv']\n",
    "\n",
    "# #Training component\n",
    "JOB_NAME = f'image-classifier-train-tf-{ID}'\n",
    "MODEL_NAME = f'image-classifier-tf-{ID}'\n",
    "TRAIN_MACHINE_TYPE = 'n1-standard-4'\n",
    "# CONTAINER_URI = 'us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest'\n",
    "MODEL_SERVING_IMAGE_URI = \"europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    "# PYTHON_MODULE = 'trainer.train_model'\n",
    "ARGS=[ \"--lr=0.003\", \"--epochs=5\"]\n",
    "IMAGE_REPOSITORY = f\"vision-{ID}\"\n",
    "IMAGE_NAME=\"image-classifier\"\n",
    "IMAGE_TAG=\"v1\"\n",
    "IMAGE_URI=f\"europe-west4-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "# #Evaluation component\n",
    "# METRICS_URI = f\"gs://{BUCKET_NAME}/deliverables/metrics.json\"\n",
    "# AVG_PR_THRESHOLD = 0.8\n",
    "# AVG_PR_CONDITION = 'avg_pr_condition'\n",
    "\n",
    "# #endpoint\n",
    "# ENDPOINT_NAME = 'fraudfinder_xgb_prediction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertex AI client\n",
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'europe-west4'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pipelines/components'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMPONENTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p -m 777 $PIPELINE_DIR $COMPONENTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define feature store component\n",
    "\n",
    "Notice that the component assumes that containes the entities-timestamps \"query\" is already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil ubla set on gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @component(output_component_file=INGEST_FEATURE_STORE, \n",
    "#            base_image=BASE_IMAGE, \n",
    "#            packages_to_install=[\"git+https://github.com/googleapis/python-aiplatform.git@main\"])\n",
    "\n",
    "# def ingest_features_gcs(project_id:str, region:str, bucket_name:str,\n",
    "#                        feature_store_id: str, read_instances_uri:str) -> NamedTuple(\"Outputs\",\n",
    "#                                                                        [(\"snapshot_uri_paths\", str),],):\n",
    "    \n",
    "#     # Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "#     from datetime import datetime\n",
    "#     import glob\n",
    "#     import urllib\n",
    "#     import json\n",
    "    \n",
    "#     #Feature Store\n",
    "#     from google.cloud.aiplatform import Featurestore, EntityType, Feature\n",
    "    \n",
    "#     # Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "#     api_endpoint = region + \"-aiplatform.googleapis.com\"\n",
    "#     bucket = urllib.parse.urlsplit(bucket_name).netloc\n",
    "#     export_uri = f'{bucket_name}/data/snapshots/{timestamp}' #format as new gsfuse requires\n",
    "#     export_uri_path = f'/gcs/{bucket}/data/snapshots/{timestamp}'\n",
    "#     customer_entity = 'customer'\n",
    "#     terminal_entity = 'terminal'\n",
    "#     serving_feature_ids = {customer_entity: ['*'], terminal_entity: ['*']}\n",
    "    \n",
    "#     # Main -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#     ## Define the feature store resource path\n",
    "#     feature_store_resource_path = f\"projects/{project_id}/locations/{region}/featurestores/{feature_store_id}\"\n",
    "#     print(\"Feature Store: \\t\", feature_store_resource_path)\n",
    "    \n",
    "#     ## Run batch job request\n",
    "#     try:\n",
    "#         ff_feature_store = Featurestore(feature_store_resource_path)\n",
    "#         ff_feature_store.batch_serve_to_gcs(\n",
    "#             gcs_destination_output_uri_prefix = export_uri,\n",
    "#             gcs_destination_type = 'csv',\n",
    "#             serving_feature_ids = serving_feature_ids,\n",
    "#             read_instances_uri = read_instances_uri,\n",
    "#             pass_through_fields = ['tx_fraud','tx_amount']\n",
    "#         )\n",
    "#     except Exception as error:\n",
    "#         print(error)\n",
    "    \n",
    "#     #Store metadata\n",
    "#     snapshot_pattern = f'{export_uri_path}/*.csv'\n",
    "#     snapshot_files = glob.glob(snapshot_pattern)\n",
    "#     snapshot_files_fmt = [p.replace('/gcs/', 'gs://') for p in snapshot_files]\n",
    "#     snapshot_files_string = json.dumps(snapshot_files_fmt)\n",
    "    \n",
    "#     component_outputs = NamedTuple(\"Outputs\",\n",
    "#                                 [(\"snapshot_uri_paths\", str),],)\n",
    "    \n",
    "#     return component_outputs(snapshot_files_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an evaluate custom component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @component(\n",
    "#     output_component_file=EVALUATE\n",
    "# )\n",
    "# def evaluate_model(\n",
    "#     model_in: Input[Artifact],\n",
    "#     metrics_uri: str,\n",
    "#     meta_metrics: Output[Metrics],\n",
    "#     graph_metrics: Output[ClassificationMetrics],\n",
    "#     model_out: Output[Artifact]) -> NamedTuple(\"Outputs\",\n",
    "#                                             [(\"metrics_thr\", float),],):\n",
    "    \n",
    "#     # Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "#     import json\n",
    "    \n",
    "#     # Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "#     metrics_path = metrics_uri.replace('gs://', '/gcs/')\n",
    "#     labels = ['not fraud', 'fraud']\n",
    "    \n",
    "#     # Main -------------------------------------------------------------------------------------------------------------------------------\n",
    "#     with open(metrics_path, mode='r') as json_file:\n",
    "#         metrics = json.load(json_file)\n",
    "\n",
    "#     ## metrics\n",
    "#     fpr = metrics['fpr']\n",
    "#     tpr = metrics['tpr']\n",
    "#     thrs = metrics['thrs']\n",
    "#     c_matrix = metrics['confusion_matrix']\n",
    "#     avg_precision_score = metrics['avg_precision_score']\n",
    "#     f1 = metrics['f1_score']\n",
    "#     lg_loss = metrics['log_loss']\n",
    "#     prec_score = metrics['precision_score']\n",
    "#     rec_score = metrics['recall_score']\n",
    "    \n",
    "#     meta_metrics.log_metric('avg_precision_score', avg_precision_score)\n",
    "#     meta_metrics.log_metric('f1_score', f1)\n",
    "#     meta_metrics.log_metric('log_loss', lg_loss)\n",
    "#     meta_metrics.log_metric('precision_score', prec_score)\n",
    "#     meta_metrics.log_metric('recall_score', rec_score)\n",
    "#     graph_metrics.log_roc_curve(fpr, tpr, thrs)\n",
    "#     graph_metrics.log_confusion_matrix(labels, c_matrix)\n",
    "    \n",
    "#     ## model metadata\n",
    "#     model_framework = 'xgb.dask'\n",
    "#     model_type = 'DaskXGBClassifier'\n",
    "#     model_user = 'author' \n",
    "#     model_function = 'classification'\n",
    "#     model_out.metadata[\"framework\"] = model_framework\n",
    "#     model_out.metadata[\"type\"] = model_type\n",
    "#     model_out.metadata[\"model function\"] = model_function\n",
    "#     model_out.metadata[\"modified by\"] = model_user\n",
    "    \n",
    "#     component_outputs = NamedTuple(\"Outputs\",\n",
    "#                                 [(\"metrics_thr\", float),],)\n",
    "    \n",
    "#     return component_outputs(float(avg_precision_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline using ```kfp``` and ```google_cloud_pipeline_components```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=PIPELINE_NAME,\n",
    ")\n",
    "def pipeline(project_id:str = PROJECT_ID, \n",
    "             region:str = REGION, \n",
    "             bucket_name:str = f\"gs://{BUCKET_NAME}\",\n",
    "             replica_count:int = 1,\n",
    "             machine_type:str = \"n1-standard-4\",\n",
    "            ):\n",
    "    \n",
    "    #create dataset \n",
    "    dataset_create_op = vertex_ai_components.ImageDatasetCreateOp(project=project_id,\n",
    "                                                       location=region,\n",
    "                                                       display_name=DATASET_NAME,\n",
    "                                                       import_schema_uri=vertex_ai.schema.dataset.ioformat.image.single_label_classification,\n",
    "                                                       gcs_source=f\"gs://{BUCKET_NAME}/prod/flowers.csv\")\n",
    "    \n",
    "    #custom training job component - script\n",
    "    train_model_op = vertex_ai_components.CustomContainerTrainingJobRunOp(\n",
    "        display_name=JOB_NAME,\n",
    "        model_display_name=MODEL_NAME,\n",
    "        container_uri=IMAGE_URI,\n",
    "        staging_bucket=bucket_name,\n",
    "        dataset=dataset_create_op.outputs['dataset'],\n",
    "        annotation_schema_uri=vertex_ai.schema.dataset.annotation.image.classification,\n",
    "        base_output_dir=bucket_name,\n",
    "        args = ARGS,\n",
    "        replica_count= replica_count,\n",
    "        machine_type= machine_type,\n",
    "        accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "        accelerator_count=1,\n",
    "        model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    "        project=project_id,\n",
    "        location=region).after(dataset_create_op)\n",
    "    \n",
    "    batch_op = ModelBatchPredictOp(\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        job_display_name=\"batch_predict_job\",\n",
    "        model=train_model_op.outputs[\"model\"],\n",
    "        gcs_source_uris=[f\"gs://{BUCKET_NAME}/test2.jsonl\"],\n",
    "        gcs_destination_output_uri_prefix=f\"gs://{BUCKET_NAME}\",\n",
    "        instances_format=\"jsonl\",\n",
    "        predictions_format=\"jsonl\",\n",
    "        model_parameters={},\n",
    "        machine_type=machine_type,\n",
    "        starting_replica_count=1,\n",
    "        max_replica_count=1,\n",
    "    )\n",
    "\n",
    "    \n",
    "    #create endpoint\n",
    "    create_endpoint_op = vertex_ai_components.EndpointCreateOp(\n",
    "        display_name=ENDPOINT_NAME,\n",
    "        project=project_id, \n",
    "        location=region).after(train_model_op)\n",
    "\n",
    "    #deploy th model\n",
    "    custom_model_deploy_op = vertex_ai_components.ModelDeployOp(\n",
    "        model=train_model_op.outputs[\"model\"],\n",
    "        endpoint=create_endpoint_op.outputs[\"endpoint\"],\n",
    "        deployed_model_display_name=MODEL_NAME,\n",
    "        dedicated_resources_machine_type=machine_type,\n",
    "        dedicated_resources_min_replica_count=replica_count\n",
    "    ).after(create_endpoint_op)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_compiler = compiler.Compiler()\n",
    "pipeline_compiler.compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate pipeline representation\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    location=REGION,\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil uniformbucketlevelaccess set on gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/446303513828/locations/europe-west4/pipelineJobs/vision-workshop-tf-pipeline-7l3oe-20221118123432\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/446303513828/locations/europe-west4/pipelineJobs/vision-workshop-tf-pipeline-7l3oe-20221118123432')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/vision-workshop-tf-pipeline-7l3oe-20221118123432?project=446303513828\n",
      "PipelineJob projects/446303513828/locations/europe-west4/pipelineJobs/vision-workshop-tf-pipeline-7l3oe-20221118123432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/446303513828/locations/europe-west4/pipelineJobs/vision-workshop-tf-pipeline-7l3oe-20221118123432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/446303513828/locations/europe-west4/pipelineJobs/vision-workshop-tf-pipeline-7l3oe-20221118123432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/446303513828/locations/europe-west4/pipelineJobs/vision-workshop-tf-pipeline-7l3oe-20221118123432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/446303513828/locations/europe-west4/pipelineJobs/vision-workshop-tf-pipeline-7l3oe-20221118123432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/446303513828/locations/europe-west4/pipelineJobs/vision-workshop-tf-pipeline-7l3oe-20221118123432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/446303513828/locations/europe-west4/pipelineJobs/vision-workshop-tf-pipeline-7l3oe-20221118123432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/446303513828/locations/europe-west4/pipelineJobs/vision-workshop-tf-pipeline-7l3oe-20221118123432 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.run(sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "{TODO: Include commands to delete individual resources below}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete endpoint resource\n",
    "! gcloud ai endpoints delete $ENDPOINT_NAME --quiet --region $REGION_NAME\n",
    "\n",
    "# Delete model resource\n",
    "! gcloud ai models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r $JOB_DIR"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
