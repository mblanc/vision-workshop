{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Fraudfinder - Training formalization\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?&download_url=https://github.com/GoogleCloudPlatform/fraudfinder/raw/main/05_model_training_formalization.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/fraudfinder/blob/main/05_model_training_formalization.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/fraudfinder/blob/main/05_model_training_formalization.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to get your training dataset from Vertex AI Feature Store, train your model using Vertex AI managed training pipeline, and deploy it as a Vertex AI endpoint. You will learn how to use your own custom code for ML training on Vertex AI.\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "- [BigQuery](https://cloud.google.com/bigquery/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "* build a container to run your own custom code on Vertex AI\n",
    "* use Vertex AI to train your model at scale\n",
    "* use Vertex AI to create an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* BigQuery\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook.### Load config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUCKET_NAME          = \"temp-vision-workshop-vision-workshop\"\n",
      "PROJECT              = \"temp-vision-workshop\"\n",
      "REGION               = \"europe-west4\"\n",
      "ID                   = \"7l3oe\"\n",
      "MODEL_NAME           = \"vision_workshop_model\"\n",
      "ENDPOINT_NAME        = \"vision_workshop_endpoint\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-vision-workshop\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "END_DATE_TRAIN = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "## Custom Training\n",
    "DATASET_NAME=f\"sample_train-{ID}-{END_DATE_TRAIN}\"\n",
    "TRAIN_JOB_NAME=f\"vision_train_frmlz-{ID}\"\n",
    "MODEL_NAME=f\"vision_model_frmlz-{ID}\"\n",
    "DEPLOYED_NAME = f\"vision_prediction_frmlz-{ID}\"\n",
    "MODEL_SERVING_IMAGE_URI = \"europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    "IMAGE_REPOSITORY = f\"vision-{ID}\"\n",
    "IMAGE_NAME=\"image-classifier\"\n",
    "IMAGE_TAG=\"v1\"\n",
    "IMAGE_URI=f\"europe-west4-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "TRAIN_COMPUTE=\"e2-standard-4\"\n",
    "DEPLOY_COMPUTE=\"n1-standard-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vision-7l3oe'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_REPOSITORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Builing a custom fraud detection model\n",
    "\n",
    "### Fixing an imbalanced dataset\n",
    "In the real world, we need to deal with imbalance in our dataset. For example, we might randomly delete some of the non-fraudulent transcations in order to approximately match the number of fraudulent transactions. This technique is called undersampling. \n",
    "\n",
    "For this workshop, we will skip the data balance process, because our sample data is small and further reduction will compromise the quality of our results. If you have a larger sample and would like to balance your data, you can uncomment and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.parse import urlparse\n",
    "# obj_list = gcs_list(TRAIN_DATA_URI)\n",
    "\n",
    "# TRAIN_DATA_URI_BALANCED = f\"{DATA_URI}/balanced_train\"\n",
    "# TRAIN_DATA_DIR_BALANCED = os.path.join(DATA_DIR, \"balanced_train\")\n",
    "# if not os.path.exists(TRAIN_DATA_DIR_BALANCED):\n",
    "#     os.makedirs(TRAIN_DATA_DIR_BALANCED)\n",
    "# for ds_csv_uri in obj_list:\n",
    "#     tx_df = pd.read_csv(ds_csv_uri)\n",
    "#     shuffled_df = tx_df.sample(frac=1,random_state=4)\n",
    "#     fraud_df = shuffled_df.loc[shuffled_df[\"tx_fraud\"] == 1]\n",
    "#     non_fraud_df = shuffled_df.loc[shuffled_df[\"tx_fraud\"] == 0].sample(n=fraud_df.shape[0],random_state=42)\n",
    "#     balanced_df = pd.concat([fraud_df, non_fraud_df])\n",
    "#     balanced_df.to_csv(os.path.join(TRAIN_DATA_DIR_BALANCED,  os.path.basename(urlparse(ds_csv_uri).path)), index=False)\n",
    "#     break\n",
    "\n",
    "# !gsutil cp -r  $TRAIN_DATA_DIR_BALANCED $TRAIN_DATA_URI_BALANCED\n",
    "# obj_list = gcs_list(TRAIN_DATA_URI_BALANCED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Builing a Vertex AI dataset\n",
    "In this section, we will build a Vertex AI dataset from our tabular data. Vertex AI datasets can be used to train AutoML models or custom-trained models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_gcs_dataset(client=vertex_ai, display_name=DATASET_NAME, gcs_source=obj_list[0]) #obj_list\n",
    "\n",
    "print(\"Dataset:\", f\"{dataset.display_name}\")\n",
    "print(\"Name: \\t\", f\"{dataset.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client() \n",
    "\n",
    "# Implicit environment set up\n",
    "# with explicit set up:\n",
    "# client = storage.Client.from_service_account_json('key-file-location')\n",
    "\n",
    "blobs = list(client.list_blobs(BUCKET_NAME, prefix='flowers/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [os.path.split(os.path.dirname(blob.name))[1] for blob in blobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [[f\"gs://{blob.bucket.name}/{blob.name}\", os.path.split(os.path.dirname(blob.name))[1]] for blob in blobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"gs://{BUCKET_NAME}/flowers/flowers.csv\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = vertex_ai.ImageDataset.create(\n",
    "    display_name=\"flowers\",\n",
    "    gcs_source=f\"gs://{BUCKET_NAME}/flowers/flowers.csv\",\n",
    "    import_schema_uri=vertex_ai.schema.dataset.ioformat.image.single_label_classification,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "ds.wait()\n",
    "\n",
    "print(ds.display_name)\n",
    "print(ds.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageDataset.list(filter=\"display_name=flowers\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a custom model\n",
    "In this section, we will use the xgboost algorithm. Specifically, we will perform custom training with a pre-built xgboost container.\n",
    "\n",
    "#### Create the training application\n",
    "Typically, to perform custom training you can use either a pre-built container or buid your own container. In this section we will build a container for xgboost, and use it to train a model with the Vertex AI Managed Training service.\n",
    "\n",
    "The first step is to write your training code. Then, write a Dockerfile and build a container image based on it. The following cell, writes our code into `train_gb.py` which is the module for training a XGBClassifier. We will copy this code later into our container to be run through Vertex Training service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p -m 777 build_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting build_training/train_tf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_training/train_tf.py\n",
    "\n",
    "\"\"\"\n",
    "train_gb.py is the module for training a XGBClassifier pipeline\n",
    "\"\"\"\n",
    "\n",
    "# Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from pathlib import Path\n",
    "\n",
    "# Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "## Read environmental variables\n",
    "TRAINING_DATA_PATH = os.environ[\"AIP_TRAINING_DATA_URI\"].replace(\"gs://\", \"/gcs/\")\n",
    "TEST_DATA_PATH = os.environ[\"AIP_TEST_DATA_URI\"].replace(\"gs://\", \"/gcs/\")\n",
    "MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"].replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Helpers -----------------------------------------------------------------------------------------------------------------------------\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data files arguments\n",
    "    parser.add_argument(\"--data_dir\", dest=\"data_dir\", type=str,\n",
    "                        required=True, help=\"Bucket uri\")\n",
    "    parser.add_argument(\"--lr\", dest=\"lr\",\n",
    "                        default=6, type=float,\n",
    "                        help=\"max_depth value.\")\n",
    "    parser.add_argument(\"-v\", \"--verbose\", \n",
    "                        help=\"increase output verbosity\", \n",
    "                        action=\"store_true\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def set_logging():\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "# def evaluate_model(model, x_true, y_true):\n",
    "    \n",
    "#     y_true = y_true.compute()\n",
    "    \n",
    "#     #calculate metrics\n",
    "#     metrics={}\n",
    "    \n",
    "#     y_score =  model.predict_proba(x_true)[:, 1]\n",
    "#     y_score = y_score.compute()\n",
    "#     fpr, tpr, thr = roc_curve(\n",
    "#          y_true=y_true, y_score=y_score, pos_label=True\n",
    "#     )\n",
    "#     fpr_list = fpr.tolist()[::50]\n",
    "#     tpr_list = tpr.tolist()[::50]\n",
    "#     thr_list = thr.tolist()[::50]\n",
    "\n",
    "#     y_pred = model.predict(x_true)\n",
    "#     y_pred.compute()\n",
    "#     c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "#     avg_precision_score = round(average_precision_score(y_true, y_score), 3)\n",
    "#     f1 = round(f1_score(y_true, y_pred), 3)\n",
    "#     lg_loss = round(log_loss(y_true, y_pred), 3)\n",
    "#     prec_score = round(precision_score(y_true, y_pred), 3)\n",
    "#     rec_score = round(recall_score(y_true, y_pred), 3)\n",
    "    \n",
    "    \n",
    "#     metrics[\"fpr\"] = [round(f, 3) for f in fpr_list]\n",
    "#     metrics[\"tpr\"] = [round(f, 3) for f in tpr_list]\n",
    "#     metrics[\"thrs\"] = [round(f, 3) for f in thr_list]\n",
    "#     metrics[\"confusion_matrix\"] = c_matrix.tolist()\n",
    "#     metrics[\"avg_precision_score\"] = avg_precision_score\n",
    "#     metrics[\"f1_score\"] = f1\n",
    "#     metrics[\"log_loss\"] = lg_loss\n",
    "#     metrics[\"precision_score\"] = prec_score\n",
    "#     metrics[\"recall_score\"] = rec_score\n",
    "    \n",
    "#     return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    if args.verbose:\n",
    "        set_logging()\n",
    "        \n",
    "    #variables\n",
    "    data_dir = args.data_dir.replace(\"gs://\", \"/gcs/\")\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    #read data\n",
    "    print(f\"TRAINING_DATA_PATH: {TRAINING_DATA_PATH}\")\n",
    "    batch_size = 16\n",
    "    img_height = 384\n",
    "    img_width = 384\n",
    "    \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    #train model\n",
    "    base_model = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\", trainable=False)\n",
    "    # Create new model on top\n",
    "    inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n",
    "    data_augmentation = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"), \n",
    "            tf.keras.layers.RandomRotation(0.1),\n",
    "            tf.keras.layers.RandomTranslation(0, 0.2),\n",
    "            tf.keras.layers.RandomTranslation(0.2, 0),\n",
    "            tf.keras.layers.RandomZoom(0.2, 0.2),\n",
    "        ]\n",
    "    )\n",
    "    x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "    x = tf.keras.layers.Rescaling(1./255)(x)\n",
    "    # The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "    # when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "    # base_model is running in inference mode here.\n",
    "    x = base_model(x)\n",
    "    #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "    outputs = tf.keras.layers.Dense(5)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=5, min_lr=0.0001)\n",
    "    history = model.fit(\n",
    "      train_ds,\n",
    "      epochs=20, \n",
    "      callbacks=[reduce_lr]\n",
    "    )\n",
    "    \n",
    "    if not Path(MODEL_DIR).exists():\n",
    "        Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    model.save(MODEL_DIR)\n",
    "    \n",
    "    #generate metrics\n",
    "    # metrics = evaluate_model(model, x_true, y_true)\n",
    "    # if not Path(deliverable_uri).exists():\n",
    "    #     Path(deliverable_uri).mkdir(parents=True, exist_ok=True)\n",
    "    # with open(metrics_uri, \"w\") as file:\n",
    "    #     json.dump(metrics, file, sort_keys = True, indent = 4)\n",
    "    # file.close()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a custom image for dask model training\n",
    "\n",
    "Here we will build a custom container. A custom container is a Docker image that you create to run your training application. By running your machine learning (ML) training job in a custom container, you can use ML frameworks, non-ML dependencies, libraries, and binaries that are not otherwise supported on Vertex AI. In othere word, we package training code on our local machine into a Docker container image, push the container image to Container Registry, and create a CustomJob.\n",
    "\n",
    "For the ML framework we will use xgboost. We also use dask and scikit libraries. Dask is an open source library for parallel computing written in Python. We will use Dask to speed up pre-processing of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n",
      "Listing items under project temp-vision-workshop, across all locations.\n",
      "\n",
      "                                                                                ARTIFACT_REGISTRY\n",
      "REPOSITORY    FORMAT  MODE                 DESCRIPTION                              LOCATION      LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
      "vision-7l3oe  DOCKER  STANDARD_REPOSITORY  Vision Workshop Docker Image repository  europe-west4          Google-managed key  2022-11-07T18:05:03  2022-11-08T09:39:25  1581.483\n"
     ]
    }
   ],
   "source": [
    "# Create image repo\n",
    "!gcloud artifacts repositories create $IMAGE_REPOSITORY \\\n",
    "    --repository-format=docker \\\n",
    "    --location=europe-west4 \\\n",
    "    --description=\"Vision Workshop Docker Image repository\"\n",
    "\n",
    "# List repositories under the project\n",
    "!gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"europe-west4-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: europe-west4-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker europe-west4-docker.pkg.dev -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting build_training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_training/Dockerfile\n",
    "# Specifies base image and tag\n",
    "# TO DO: to change it to python-slim\n",
    "FROM python:3.7 \n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip install gcsfs numpy pandas scikit-learn tensorflow==2.8.3 tensorflow_datasets tensorflow_hub google-cloud-aiplatform --upgrade\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY ./train_tf.py /root/train_tf.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"train_tf.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  8.192kB\n",
      "Step 1/5 : FROM python:3.7\n",
      " ---> c643d609e965\n",
      "Step 2/5 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> 52a8a881f1bd\n",
      "Step 3/5 : RUN pip install gcsfs numpy pandas scikit-learn tensorflow==2.8.3 tensorflow_datasets tensorflow_hub google-cloud-aiplatform --upgrade\n",
      " ---> Using cache\n",
      " ---> 4546d8d64b09\n",
      "Step 4/5 : COPY ./train_tf.py /root/train_tf.py\n",
      " ---> 06d45e886550\n",
      "Step 5/5 : ENTRYPOINT [\"python3\", \"train_tf.py\"]\n",
      " ---> Running in a9a9dc9c224f\n",
      "Removing intermediate container a9a9dc9c224f\n",
      " ---> 1319b167ba62\n",
      "Successfully built 1319b167ba62\n",
      "Successfully tagged europe-west4-docker.pkg.dev/temp-vision-workshop/vision-7l3oe/image-classifier:v1\n",
      "The push refers to repository [europe-west4-docker.pkg.dev/temp-vision-workshop/vision-7l3oe/image-classifier]\n",
      "\n",
      "\u001b[1B5e0fd0a4: Preparing \n",
      "\u001b[1Bc06c4dfc: Preparing \n",
      "\u001b[1Bf78a199d: Preparing \n",
      "\u001b[1Bc04719be: Preparing \n",
      "\u001b[1B8b761eaf: Preparing \n",
      "\u001b[1B3c62e3d7: Preparing \n",
      "\u001b[1Bd36bfd35: Preparing \n",
      "\u001b[1Bc9917839: Preparing \n",
      "\u001b[1Bdf39e1dd: Preparing \n",
      "\u001b[1B21b04368: Preparing \n",
      "\u001b[11Be0fd0a4: Pushed lready exists 7kB\u001b[9A\u001b[2K\u001b[3A\u001b[2K\u001b[11A\u001b[2Kv1: digest: sha256:d40ebea2c280a7e8109afe1542b4bf777874d7ee265e2daf3fb5a8a896faee75 size: 2641\n"
     ]
    }
   ],
   "source": [
    "# Build and push docker file\n",
    "!docker build -t $IMAGE_URI ./build_training/\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the script to run on Vertex AI\n",
    "In this section, we create a training pipeline. It will create custom training jobs, load our dataset and upload the model to Vertex AI after the training job is successfully completed. Learn more about creating of custom jobs [here](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.datasets.image_dataset.ImageDataset object at 0x7f82d26fb8d0> \n",
       "resource name: projects/446303513828/locations/europe-west4/datasets/4893719547044954112"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME, experiment=\"train\")\n",
    "\n",
    "\n",
    "ds = vertex_ai.ImageDataset.list(filter=\"display_name=flowers\")[0]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://temp-vision-workshop-vision-workshop/aiplatform-custom-training-2022-11-08-10:24:52.496 \n",
      "No dataset split provided. The service will use a default split.\n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west4/training/2086418971216576512?project=446303513828\n",
      "CustomContainerTrainingJob projects/446303513828/locations/europe-west4/trainingPipelines/2086418971216576512 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west4/training/5948466757919309824?project=446303513828\n",
      "CustomContainerTrainingJob projects/446303513828/locations/europe-west4/trainingPipelines/2086418971216576512 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/446303513828/locations/europe-west4/trainingPipelines/2086418971216576512 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/446303513828/locations/europe-west4/trainingPipelines/2086418971216576512 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/446303513828/locations/europe-west4/trainingPipelines/2086418971216576512 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/446303513828/locations/europe-west4/trainingPipelines/2086418971216576512 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/446303513828/locations/europe-west4/trainingPipelines/2086418971216576512 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/446303513828/locations/europe-west4/trainingPipelines/2086418971216576512 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job = vertex_ai.CustomContainerTrainingJob(\n",
    "    display_name=TRAIN_JOB_NAME,\n",
    "    container_uri=IMAGE_URI,\n",
    "    model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    ")\n",
    "\n",
    "parameters = {\"LR\": 0.003}\n",
    "\n",
    "CMDARGS = [ f\"\"\"--data_dir=gs://{BUCKET_NAME}/flowers\"\"\",\n",
    "    \"--lr=\" + str(parameters[\"LR\"]),\n",
    "    \"--verbose\"\n",
    "]\n",
    "\n",
    "\n",
    "model = job.run(\n",
    "    dataset=ds,\n",
    "    annotation_schema_uri=vertex_ai.schema.dataset.annotation.image.classification,\n",
    "    model_display_name=MODEL_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can run the model via an endpoint, you need to preprocess it to match the format that your custom model defined in task.py expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"tx_fraud\"\n",
    "UNUSED_COLUMNS = [\"timestamp\",\"entity_type_customer\",\"entity_type_terminal\"]\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "def preprocess(df):\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df with raw data\n",
    "\n",
    "    Returns:\n",
    "      df with preprocessed data\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    # Drop rows with NaN's\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
    "\n",
    "    dummy_columns = list(df.dtypes[df.dtypes == \"category\"].index)\n",
    "    df = pd.get_dummies(df, columns=dummy_columns)\n",
    "\n",
    "    return df\n",
    "#test set\n",
    "train_sample_path = os.path.join(TRAIN_DATA_DIR, \"000000000000.csv\")\n",
    "df_test = pd.read_csv(train_sample_path)\n",
    "preprocessed_test_Data = preprocess(df_test)\n",
    "\n",
    "x_test = preprocessed_test_Data[preprocessed_test_Data.columns.drop(LABEL_COLUMN).to_list()].values\n",
    "y_test = preprocessed_test_Data.loc[:,LABEL_COLUMN].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we copy the model artifact to the local directory to evaluate the model localy before deploying the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r $model.uri ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the folliwng cell, we use the test dataset, to predict the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.Booster()  # init model\n",
    "bst.load_model(\"./model/model.bst\") \n",
    "xgtest = xgb.DMatrix(x_test)\n",
    "y_pred_prob = bst.predict(xgtest)\n",
    "y_pred = y_pred_prob.round().astype(int)\n",
    "y_pred_prob[0:10]\n",
    "precision_recall_fscore_support(y_test.values, y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model\n",
    "Before you use your model to make predictions, you need to deploy it to an Endpoint. You can do this by calling the deploy function on the Model resource. This will do two things:\n",
    "\n",
    "- create an Endpoint resource\n",
    "- deploy the Model resource to the Endpoint resource\n",
    "\n",
    "TO DO: PRE-DEPLOYING THIS MODEL SO USER DOESN'T HAVE TO WAIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_COMPUTE=\"n1-standard-4\"\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    accelerator_count=0,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the deployed model (Make an online prediction request)\n",
    "Send an online prediction request to your deployed model. To make sure your deployed model is working, test it out by sending a request to the endpoint.\n",
    "\n",
    "Let's first get a test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"instances\": x_test[:2].tolist()\n",
    "  \n",
    "}\n",
    "\n",
    "# In case you want to test it in the console\n",
    "import json\n",
    "with open(\"predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.predict(instances = payload[\"instances\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (DO NOT RUN) Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint resource\n",
    "#! gcloud ai endpoints delete $ENDPOINT_NAME --quiet --region $REGION_NAME\n",
    "\n",
    "# Delete model resource\n",
    "#! gcloud ai models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "#! gsutil -m rm -r $JOB_DIR"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
