{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vision - Training formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "[Vision Workshop](https://github.com/mblanc/vision-workshop) is a series of labs on how to build an image classification system on Google Cloud. Throughout the Vision Workshop labs, you will learn how to read image data stored in data lake, perform exploratory data analysis (EDA), train a model, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to pull features from Feature Store for training, run data exploratory analysis on features, build a machine learning model locally, experiment with various hyperparameters, evaluate the model and deloy it to a Vertex AI endpoint. \n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "- Do some exploratory analysis on the extracted data\n",
    "- Train the model and track the results using Vertex AI Training\n",
    "- Deploy the Model using Vertex AI Prediction\n",
    "- Launch a Batch predictions job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook.### Load config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-vision-workshop\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "END_DATE_TRAIN = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "## Custom Training\n",
    "DATASET_NAME=f\"sample_train-{ID}-{END_DATE_TRAIN}\"\n",
    "TRAIN_JOB_NAME=f\"vision_train_frmlz-{ID}\"\n",
    "MODEL_NAME=f\"vision_model_frmlz-{ID}\"\n",
    "DEPLOYED_NAME = f\"vision_prediction_frmlz-{ID}\"\n",
    "MODEL_SERVING_IMAGE_URI = \"europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    "IMAGE_REPOSITORY = f\"vision-{ID}\"\n",
    "IMAGE_NAME=\"image-classifier\"\n",
    "IMAGE_TAG=\"v1\"\n",
    "IMAGE_URI=f\"europe-west4-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "TRAIN_COMPUTE=\"n1-standard-4\"\n",
    "DEPLOY_COMPUTE=\"n1-standard-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builing a custom Image Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a custom model\n",
    "In this section, we will use the tensorflow algorithm. Specifically, we will perform custom training with a custom tensorflow container.\n",
    "\n",
    "#### Create the training application\n",
    "Typically, to perform custom training you can use either a pre-built container or buid your own container. In this section we will build a container for tensorflow, and use it to train a model with the Vertex AI Managed Training service.\n",
    "\n",
    "The first step is to write your training code. Then, write a Dockerfile and build a container image based on it. The following cell, writes our code into `train_tf.py` which is the module for training a Tensorflow model. We will copy this code later into our container to be run through Vertex Training service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p -m 777 build_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some noteable steps include:\n",
    "\n",
    "- Command-line arguments:\n",
    "    - `train-data-dir`, `val-data-dir`, `test-data-dir`: The Cloud Storage locations of the train, validation and test data. When using Vertex AI custom training, these locations will be specified in the corresponding environment variables: `AIP_TRAINING_DATA_URI`, `AIP_VALIDATION_DATA_URI`, and `AIP_TEST_DATA_URI`. The data is exported from an `ImageDataSet` and will be in a JSONL format.\n",
    "    - `model-dir`: The location to save the trained model. When using Vertex AI custom training, the location will be specified in the environment variable: `AIP_MODEL_DIR`,\n",
    "- Data preprocessing (`get_data()`):\n",
    "    - Compiles the one or more JSONL data files for a dataset, and constructs a `tf.data.Dataset()` generator for data preprocessing and model feeding.\n",
    "- Model architecture (`get_model()`):\n",
    "    - Builds the corresponding model architecture.\n",
    "- Training (`train_model()`):\n",
    "    - Trains the model\n",
    "- Model artifact saving\n",
    "    - Saves the model artifacts where the Cloud Storage location is determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_training/train_tf.py\n",
    "\n",
    "\"\"\"\n",
    "train_tf.py is the module for training a Tensorflow Image classifier pipeline\n",
    "\"\"\"\n",
    "\n",
    "# Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from pathlib import Path\n",
    "\n",
    "# Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "## Read environmental variables\n",
    "TRAINING_DATA_PATH = os.environ[\"AIP_TRAINING_DATA_URI\"].replace(\"gs://\", \"/gcs/\")\n",
    "TEST_DATA_PATH = os.environ[\"AIP_TEST_DATA_URI\"].replace(\"gs://\", \"/gcs/\")\n",
    "MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"].replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Helpers -----------------------------------------------------------------------------------------------------------------------------\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data files arguments\n",
    "    parser.add_argument(\"--train-data-dir\", default=os.getenv('AIP_TRAINING_DATA_URI'), dest=\"train_data_dir\", type=str, help=\"train data directory\")\n",
    "    parser.add_argument(\"--val-data-dir\", default=os.getenv('AIP_VALIDATION_DATA_URI'), dest=\"val_data_dir\", type=str, help=\"validation data directory\")\n",
    "    \n",
    "    # data preprocessing\n",
    "    parser.add_argument(\"--image-width\", dest=\"image_width\", default=384, type=int, help=\"image width\")\n",
    "    parser.add_argument(\"--image-height\", dest=\"image_height\", default=384, type=int, help=\"image height\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\"--lr\", dest=\"lr\", default=0.01, type=float, help=\"learning rate.\")\n",
    "    parser.add_argument(\"--batch-size\", dest=\"batch_size\", default=32, type=int, help=\"mini-batch size\")\n",
    "    parser.add_argument(\"--epochs\", default=10, type=int, help=\"number of training epochs\")\n",
    "    parser.add_argument(\"--steps\", dest=\"steps\", default=92, type=int, help=\"Number of steps per epoch.\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "def get_data():\n",
    "    class_names = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "    class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    def parse_image(filename):\n",
    "        image = tf.io.read_file(filename)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [args.image_width, args.image_height])\n",
    "        return image\n",
    "\n",
    "    def extract(data_dir, batch_size=args.batch_size, repeat=True):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for data_uri in tf.io.gfile.glob(pattern=data_dir):\n",
    "            with tf.io.gfile.GFile(name=data_uri, mode=\"r\") as gfile:\n",
    "                for line in gfile.readlines():\n",
    "                    instance = json.loads(line)\n",
    "                    data.append(instance[\"imageGcsUri\"])\n",
    "                    classification_annotation = instance[\"classificationAnnotations\"][0]\n",
    "                    label = classification_annotation[\"displayName\"]\n",
    "                    labels.append(class_indices[label])\n",
    "\n",
    "        data_dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "        data_dataset = data_dataset.map(\n",
    "            parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        label_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
    "\n",
    "        dataset = tf.data.Dataset.zip((data_dataset, label_dataset)).cache().shuffle(batch_size * 32)\n",
    "        if repeat:\n",
    "            dataset = dataset.repeat()\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        # Add property to retain the class names\n",
    "        dataset.class_names = class_names\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    logging.info('Prepare training data')\n",
    "    train_dataset = extract(args.train_data_dir)\n",
    "\n",
    "    logging.info('Prepare validation data')\n",
    "    val_dataset = extract(args.val_data_dir, batch_size=1, repeat=False)\n",
    "\n",
    "    return num_classes, train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def get_cnn_model(num_classes):\n",
    "    logging.info(\"Get model architecture\")\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(args.image_height, args.image_width, 3)),\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(num_classes)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(num_classes):\n",
    "    logging.info(\"Get model architecture\")\n",
    "    data_augmentation = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"), \n",
    "            tf.keras.layers.RandomRotation(0.1),\n",
    "            tf.keras.layers.RandomTranslation(0, 0.2),\n",
    "            tf.keras.layers.RandomTranslation(0.2, 0),\n",
    "            tf.keras.layers.RandomZoom(0.2, 0.2),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # base model\n",
    "    base_model = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\", trainable=False)\n",
    "    # Create new model on top\n",
    "    inputs = tf.keras.Input(shape=(args.image_height, args.image_width, 3))\n",
    "    x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "    x = tf.keras.layers.Rescaling(1./255)(x)\n",
    "    # The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "    # when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "    # base_model is running in inference mode here.\n",
    "    x = base_model(x)\n",
    "    # x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "    outputs = tf.keras.layers.Dense(num_classes)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset):\n",
    "    logging.info(\"Start model training\")\n",
    "    history = model.fit(\n",
    "        x=train_dataset, epochs=args.epochs, steps_per_epoch=args.steps, batch_size=args.batch_size, validation_data=val_dataset\n",
    "    )\n",
    "    return history\n",
    "\n",
    "        \n",
    "num_classes, train_dataset, val_dataset = get_data()\n",
    "\n",
    "model = get_model(num_classes=num_classes)\n",
    "\n",
    "history = train_model(model, train_dataset, val_dataset)\n",
    "\n",
    "if not Path(MODEL_DIR).exists():\n",
    "    Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONCRETE_INPUT = \"numpy_inputs\"\n",
    "\n",
    "\n",
    "def _preprocess(bytes_input):\n",
    "    decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
    "    resized = tf.image.resize(decoded, size=(384, 384))\n",
    "    return resized\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def preprocess_fn(bytes_inputs):\n",
    "    decoded_images = tf.map_fn(\n",
    "        _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n",
    "    )\n",
    "    return {\n",
    "        CONCRETE_INPUT: decoded_images\n",
    "    }  # User needs to make sure the key matches model's input\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def serving_fn(bytes_inputs):\n",
    "    images = preprocess_fn(bytes_inputs)\n",
    "    prob = m_call(**images)\n",
    "    return prob\n",
    "\n",
    "\n",
    "m_call = tf.function(model.call).get_concrete_function(\n",
    "    [tf.TensorSpec(shape=[None, 384, 384, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\n",
    ")\n",
    "\n",
    "tf.saved_model.save(\n",
    "    model, MODEL_DIR, signatures={\n",
    "        \"serving_default\": serving_fn,\n",
    "        # Required for XAI\n",
    "        \"xai_preprocess\": preprocess_fn,\n",
    "        \"xai_model\": m_call\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Image models with serving functions\n",
    "\n",
    "Previously, your model server took input as a 3-dimensional array. Image models typically take a compressed image and use a serving function fused to the model to decompress the compressed image into a 3-dimensional array, and other preprocesing -- such as normalizing the pixel values.\n",
    "\n",
    "Next, you upload your custom image model as a `Vertex AI Model` resource with a serving function. During upload, you define a serving function to convert data to the format your model expects. If you send encoded data to Vertex AI, your serving function ensures that the data is decoded on the model server before it is passed as input to your model.\n",
    "\n",
    "##### How does the serving function work\n",
    "\n",
    "When you send a request to an online prediction server, the request is received by a HTTP server. The HTTP server extracts the prediction request from the HTTP request content body. The extracted prediction request is forwarded to the serving function. For Google pre-built prediction containers, the request content is passed to the serving function as a `tf.string`.\n",
    "\n",
    "The serving function consists of two parts:\n",
    "\n",
    "- `preprocessing function`:\n",
    "  - Converts the input (`tf.string`) to the input shape and data type of the underlying model (dynamic graph).\n",
    "  - Performs the same preprocessing of the data that was done during training the underlying model -- e.g., normalizing, scaling, etc.\n",
    "- `post-processing function`:\n",
    "  - Converts the model output to format expected by the receiving application -- e.q., compresses the output.\n",
    "  - Packages the output for the the receiving application -- e.g., add headings, make JSON object, etc.\n",
    "\n",
    "Both the preprocessing and post-processing functions are converted to static graphs which are fused to the model. The output from the underlying model is passed to the post-processing function. The post-processing function passes the converted/packaged output back to the HTTP server. The HTTP server returns the output as the HTTP response content.\n",
    "\n",
    "One consideration you need to consider when building serving functions for TF.Keras models is that they run as static graphs. That means, you cannot use TF graph operations that require a dynamic graph. If you do, you will get an error during the compile of the serving function which will indicate that you are using an EagerTensor which is not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a custom image for tensorflow model training\n",
    "\n",
    "Here we will build a custom container. A custom container is a Docker image that you create to run your training application. By running your machine learning (ML) training job in a custom container, you can use ML frameworks, non-ML dependencies, libraries, and binaries that are not otherwise supported on Vertex AI. In othere word, we package training code on our local machine into a Docker container image, push the container image to Container Registry, and create a CustomJob.\n",
    "\n",
    "For the ML framework we will use tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image repo\n",
    "!gcloud artifacts repositories create $IMAGE_REPOSITORY \\\n",
    "    --repository-format=docker \\\n",
    "    --location=europe-west4 \\\n",
    "    --description=\"Vision Workshop Docker Image repository\"\n",
    "\n",
    "# List repositories under the project\n",
    "!gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker europe-west4-docker.pkg.dev -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_training/Dockerfile\n",
    "# Specifies base image and tag\n",
    "FROM tensorflow/tensorflow:2.8.3-gpu\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install gcsfs numpy pandas scikit-learn google-cloud-aiplatform tensorflow_datasets tensorflow_hub --upgrade\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY ./train_tf.py /root/train_tf.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"train_tf.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and push docker file\n",
    "!docker build -t $IMAGE_URI ./build_training/\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the script to run on Vertex AI\n",
    "In this section, we create a training pipeline. It will create custom training jobs, load our dataset and upload the model to Vertex AI after the training job is successfully completed. Learn more about creating of custom jobs [here](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME, experiment=\"train\")\n",
    "\n",
    "\n",
    "ds = vertex_ai.ImageDataset.list(filter=\"display_name=flowers\")[0]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertex_ai.CustomContainerTrainingJob(\n",
    "    display_name=TRAIN_JOB_NAME,\n",
    "    container_uri=IMAGE_URI,\n",
    "    model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    ")\n",
    "\n",
    "parameters = {\"LR\": 0.003}\n",
    "\n",
    "CMDARGS = [ \"--lr=\" + str(parameters[\"LR\"]), \"--epochs=5\"]\n",
    "\n",
    "\n",
    "model = job.run(\n",
    "    dataset=ds,\n",
    "    annotation_schema_uri=vertex_ai.schema.dataset.annotation.image.classification,\n",
    "    model_display_name=MODEL_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\"LR\": 0.003}\n",
    "\n",
    "# CMDARGS = [ \"--lr=\" + str(parameters[\"LR\"]), \"--epochs=5\"]\n",
    "\n",
    "# job = vertex_ai.CustomTrainingJob(\n",
    "#     display_name=TRAIN_JOB_NAME,\n",
    "#     script_path=\"build_training/train_tf.py\",\n",
    "#     container_uri=\"europe-docker.pkg.dev/vertex-ai/training/tf-gpu.2-8:latest\",\n",
    "#     requirements=[\"gcsfs\", \"tensorflow_datasets\", \"tensorflow_hub\"],\n",
    "#     model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    "# )\n",
    "\n",
    "\n",
    "# model = job.run(\n",
    "#     dataset=ds,\n",
    "#     annotation_schema_uri=vertex_ai.schema.dataset.annotation.image.classification,\n",
    "#     model_display_name=MODEL_NAME,\n",
    "#     args=CMDARGS,\n",
    "#     replica_count=1,\n",
    "#     machine_type=TRAIN_COMPUTE,\n",
    "#     accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "#     accelerator_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Explanation Specification\n",
    "\n",
    "To get explanations when doing a prediction, you must enable the explanation capability and set corresponding settings when you upload your custom model to an Vertex `Model` resource. These settings are referred to as the explanation metadata, which consists of:\n",
    "\n",
    "- `parameters`: This is the specification for the explainability algorithm to use for explanations on your model. You can choose between:\n",
    "  - Shapley - *Note*, not recommended for image data -- can be very long running\n",
    "  - XRAI\n",
    "  - Integrated Gradients\n",
    "- `metadata`: This is the specification for how the algoithm is applied on your custom model.\n",
    "\n",
    "#### Explanation Parameters\n",
    "\n",
    "Let's first dive deeper into the settings for the explainability algorithm.\n",
    "\n",
    "#### Shapley\n",
    "\n",
    "Assigns credit for the outcome to each feature, and considers different permutations of the features. This method provides a sampling approximation of exact Shapley values.\n",
    "\n",
    "Use Cases:\n",
    "  - Classification and regression on tabular data.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- `path_count`: This is the number of paths over the features that will be processed by the algorithm. An exact approximation of the Shapley values requires M! paths, where M is the number of features. For the CIFAR10 dataset, this would be 784 (28*28).\n",
    "\n",
    "For any non-trival number of features, this is too compute expensive. You can reduce the number of paths over the features to M * `path_count`.\n",
    "\n",
    "#### Integrated Gradients\n",
    "\n",
    "A gradients-based method to efficiently compute feature attributions with the same axiomatic properties as the Shapley value.\n",
    "\n",
    "Use Cases:\n",
    "  - Classification and regression on tabular data.\n",
    "  - Classification on image data.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- `step_count`: This is the number of steps to approximate the remaining sum. The more steps, the more accurate the integral approximation. The general rule of thumb is 50 steps, but as you increase so does the compute time.\n",
    "\n",
    "#### XRAI\n",
    "\n",
    "Based on the integrated gradients method, XRAI assesses overlapping regions of the image to create a saliency map, which highlights relevant regions of the image rather than pixels.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "  - Classification on image data.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- `step_count`: This is the number of steps to approximate the remaining sum. The more steps, the more accurate the integral approximation. The general rule of thumb is 50 steps, but as you increase so does the compute time.\n",
    "\n",
    "In the next code cell, set the variable `XAI` to which explainabilty algorithm you will use on your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XAI = \"xrai\"  # [ shapley, ig, xrai ]\n",
    "\n",
    "if XAI == \"shapley\":\n",
    "    PARAMETERS = {\"sampled_shapley_attribution\": {\"path_count\": 10}}\n",
    "elif XAI == \"ig\":\n",
    "    PARAMETERS = {\"integrated_gradients_attribution\": {\"step_count\": 10}}\n",
    "elif XAI == \"xrai\":\n",
    "    PARAMETERS = {\"xrai_attribution\": {\"step_count\": 10}}\n",
    "\n",
    "parameters = vertex_ai.explain.ExplanationParameters(PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Explanation Metadata\n",
    "\n",
    "Let's first dive deeper into the explanation metadata, which consists of:\n",
    "\n",
    "- `outputs`: A scalar value in the output to attribute -- what to explain. For example, in a probability output \\[0.1, 0.2, 0.7\\] for classification, one wants an explanation for 0.7. Consider the following formulae, where the output is `y` and that is what we want to explain.\n",
    "\n",
    "    y = f(x)\n",
    "\n",
    "Consider the following formulae, where the outputs are `y` and `z`. Since we can only do attribution for one scalar value, we have to pick whether we want to explain the output `y` or `z`. Assume in this example the model is object detection and y and z are the bounding box and the object classification. You would want to pick which of the two outputs to explain.\n",
    "\n",
    "    y, z = f(x)\n",
    "\n",
    "The dictionary format for `outputs` is:\n",
    "\n",
    "    { \"outputs\": { \"[your_display_name]\":\n",
    "                   \"output_tensor_name\": [layer]\n",
    "                 }\n",
    "    }\n",
    "\n",
    "<blockquote>\n",
    " -  [your_display_name]: A human readable name you assign to the output to explain. A common example is \"probability\".<br/>\n",
    " -  \"output_tensor_name\": The key/value field to identify the output layer to explain. <br/>\n",
    " -  [layer]: The output layer to explain. In a single task model, like a tabular regressor, it is the last (topmost) layer in the model.\n",
    "</blockquote>\n",
    "\n",
    "- `inputs`: The features for attribution -- how they contributed to the output. Consider the following formulae, where `a` and `b` are the features. We have to pick which features to explain how the contributed. Assume that this model is deployed for A/B testing, where `a` are the data_items for the prediction and `b` identifies whether the model instance is A or B. You would want to pick `a` (or some subset of) for the features, and not `b` since it does not contribute to the prediction.\n",
    "\n",
    "    y = f(a,b)\n",
    "\n",
    "The minimum dictionary format for `inputs` is:\n",
    "\n",
    "    { \"inputs\": { \"[your_display_name]\":\n",
    "                  \"input_tensor_name\": [layer]\n",
    "                 }\n",
    "    }\n",
    "\n",
    "<blockquote>\n",
    " -  [your_display_name]: A human readable name you assign to the input to explain. A common example is \"features\".<br/>\n",
    " -  \"input_tensor_name\": The key/value field to identify the input layer for the feature attribution. <br/>\n",
    " -  [layer]: The input layer for feature attribution. In a single input tensor model, it is the first (bottom-most) layer in the model.\n",
    "</blockquote>\n",
    "\n",
    "Since the inputs to the model are tabular, you can specify the following two additional fields as reporting/visualization aids:\n",
    "\n",
    "<blockquote>\n",
    " - \"modality\": \"image\": Indicates the field values are image data.\n",
    "</blockquote>\n",
    "\n",
    "Since the inputs to the model are images, you can specify the following additional fields as reporting/visualization aids:\n",
    "\n",
    "<blockquote>\n",
    " - \"modality\": \"image\": Indicates the field values are image data.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_metadata = vertex_ai.explain.ExplanationMetadata.InputMetadata({\"input_tensor_name\": \"numpy_inputs\", \"modality\": \"image\"})\n",
    "output_metadata = vertex_ai.explain.ExplanationMetadata.OutputMetadata({\"output_tensor_name\": \"output_0\"})\n",
    "\n",
    "metadata = vertex_ai.explain.ExplanationMetadata(\n",
    "    inputs={\"image\": input_metadata}, outputs={\"class\": output_metadata}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the model\n",
    "\n",
    "Next, upload your model to a `Model` resource using `Model.upload()` method, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Model` resource.\n",
    "- `artifact`: The Cloud Storage location of the trained model artifacts.\n",
    "- `serving_container_image_uri`: The serving container image.\n",
    "- `sync`: Whether to execute the upload asynchronously or synchronously.\n",
    "- `explanation_parameters`: Parameters to configure explaining for `Model`'s predictions.\n",
    "- `explanation_metadata`: Metadata describing the `Model`'s input and output for explanation.\n",
    "\n",
    "If the `upload()` method is run asynchronously, you can subsequently block until completion with the `wait()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vertex_ai.Model.upload(\n",
    "    display_name=MODEL_NAME + \"_explanations\",\n",
    "    artifact_uri=model.uri,\n",
    "    serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    "    explanation_parameters=parameters,\n",
    "    explanation_metadata=metadata,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = vertex_ai.Model.upload(\n",
    "#     display_name=MODEL_NAME + \"_gpu\",\n",
    "#     artifact_uri=model.uri,\n",
    "#     serving_container_image_uri=\"europe-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-8:latest\",\n",
    "#     explanation_parameters=parameters,\n",
    "#     explanation_metadata=metadata,\n",
    "#     sync=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Deploy the model\n",
    "Before you use your model to make predictions, you need to deploy it to an Endpoint. You can do this by calling the deploy function on the Model resource. This will do two things:\n",
    "\n",
    "- create an Endpoint resource\n",
    "- deploy the Model resource to the Endpoint resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_COMPUTE=\"n1-standard-4\"\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    # accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    # accelerator_count=1,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    explanation_parameters=parameters,\n",
    "    explanation_metadata=metadata,\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the deployed model (Make an online prediction request)\n",
    "Send an online prediction request to your deployed model. To make sure your deployed model is working, test it out by sending a request to the endpoint.\n",
    "\n",
    "Let's first get a test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import base64\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = vertex_ai.Endpoint.list(filter=f\"display_name={MODEL_NAME}_explanations_endpoint\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vertex_ai.Model.list(filter=f\"display_name={MODEL_NAME}_explanations\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunflower_url = \"gs://temp-vision-workshop-vision-workshop/flowers/daisy/100080576_f52e8ee070_n.jpg\"\n",
    "sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n",
    "\n",
    "\n",
    "img = Image.open(sunflower_path).resize((384, 384))\n",
    "from io import BytesIO\n",
    "\n",
    "buffered = BytesIO()\n",
    "img.save(buffered, format=\"JPEG\")\n",
    "\n",
    "bytes_input = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "instances = [{'bytes_inputs': {'b64': bytes_input}}]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "predictions = endpoint.predict(instances=instances)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "endpoint.predict(instances=instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "for explanation in endpoint.explain(instances=instances).explanations:\n",
    "    attributions = dict(explanation.attributions[0].feature_attributions)\n",
    "    label_index = explanation.attributions[0].output_index[0]\n",
    "    class_name = class_names[label_index]\n",
    "    b64str = attributions[\"image\"][\"b64_jpeg\"]\n",
    "    image = base64.b64decode(b64str)\n",
    "    image = io.BytesIO(image)\n",
    "    img = mpimg.imread(image, format=\"JPG\")\n",
    "\n",
    "    plt.imshow(img, interpolation=\"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch predictions\n",
    "\n",
    "Send a batch prediction request to your deployed model.\n",
    "\n",
    "Batch prediction provides the ability to do offline batch processing of large amounts of prediction requests. Resources are only provisioned during the batch process and then deprovisioned when the batch request is completed. The results are stored in Cloud Storage, in contrast to online prediction where the results are returned as a HTTP response packet.\n",
    "\n",
    "The input format for your batch job is dependent on the format supported by your model server. Foremost, the web server in your model server must support a JSONL format, which the web server will convert to a format support either directly by the model input intertace or a serving function interface. For batch prediction, this JSONL format is referred to as the `pivot` format.\n",
    "\n",
    "### Input format for batch prediction jobs\n",
    "\n",
    "The batch server accepts the following input formats for custom image models:\n",
    "\n",
    "- JSONL\n",
    "- File-List\n",
    "\n",
    "### Output format for batch prediction jobs\n",
    "\n",
    "The batch server accepts the following output formats for custom image models:\n",
    "\n",
    "- JSONL\n",
    "\n",
    "### Pivot format\n",
    "\n",
    "The batch server converts the input format to the `pivot` (JSONL) format as follows:\n",
    "\n",
    "**JSONL**\n",
    "\n",
    "Each input line (request) should contain one and only one valid json value.\n",
    "\n",
    "    {\"values\": [1, 2, 3, 4], \"key\": 1}\n",
    "    {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
    "\n",
    "The batch server generates the pivot data with the same format. The generated pivot data is then wrapped into a payload request:\n",
    "\n",
    "    {\"instances\": [\n",
    "      {\"values\": [1, 2, 3, 4], \"key\": 1},\n",
    "      {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
    "    ]}\n",
    "\n",
    "**FileList**\n",
    "\n",
    "The FileList format contains a list of files. Each line in a “FileList” file specifies a single file path, specified as a Cloud Storage location.\n",
    "\n",
    "    gs://my-bucket/file1.txt\n",
    "    gs://my-bucket/file2.txt\n",
    "\n",
    "The batch server reads the files as binaries. The binary objects are serialized as ASCII strings.\n",
    "\n",
    "    {\"instances\": [\n",
    "     {\"b64\",\"b64EncodedASCIIString\"},\n",
    "     {\"b64\",\"b64EncodedASCIIString\"}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create such a `FileList` input file by sampling ten images per label from our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client() \n",
    "\n",
    "daisies = list(client.list_blobs(BUCKET_NAME, prefix='flowers/daisy'))[:10]\n",
    "tullips = list(client.list_blobs(BUCKET_NAME, prefix='flowers/tullip'))[:10]\n",
    "roses = list(client.list_blobs(BUCKET_NAME, prefix='flowers/rose'))[:10]\n",
    "sunflowers = list(client.list_blobs(BUCKET_NAME, prefix='flowers/sunflower'))[:10]\n",
    "dandelions = list(client.list_blobs(BUCKET_NAME, prefix='flowers/dandelion'))[:10]\n",
    "\n",
    "images = daisies + tullips + roses + sunflowers + dandelions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [f\"gs://{image.bucket.name}/{image.name}\" for image in images]\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from io import BytesIO\n",
    "\n",
    "gcs_input_uri = f\"gs://{BUCKET_NAME}/flowers_batch.txt\"\n",
    "\n",
    "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
    "    for image in images:\n",
    "        f.write(image + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "gcs_input_uri = f\"gs://{BUCKET_NAME}/test.jsonl\"\n",
    "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
    "    for image in images:\n",
    "        from io import BytesIO\n",
    "        bytes = tf.io.read_file(image)\n",
    "        img = Image.open(io.BytesIO(bytes.numpy())).resize((384, 384))\n",
    "        buffered = BytesIO()\n",
    "        img.save(buffered, format=\"JPEG\")\n",
    "        b64str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "        data = {\"bytes_inputs\": {\"b64\": b64str}}\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send the prediction request\n",
    "\n",
    "To make a batch prediction request, call the model object's `batch_predict` method with the following parameters: \n",
    "- `instances_format`: The format of the batch prediction request file: \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" or \"file-list\"\n",
    "- `prediction_format`: The format of the batch prediction response file: \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" or \"file-list\"\n",
    "- `job_display_name`: The human readable name for the prediction job.\n",
    " - `gcs_source`: A list of one or more Cloud Storage paths to your batch prediction requests.\n",
    "- `gcs_destination_prefix`: The Cloud Storage path that the service will write the predictions to.\n",
    "- `model_parameters`: Additional filtering parameters for serving prediction results.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n",
    "\n",
    "### Compute instance scaling\n",
    "\n",
    "You can specify a single instance (or node) to process your batch prediction request. This tutorial uses a single node, so the variables `MIN_NODES` and `MAX_NODES` are both set to `1`.\n",
    "\n",
    "If you want to use multiple nodes to process your batch prediction request, set `MAX_NODES` to the maximum number of nodes you want to use. Vertex AI autoscales the number of nodes used to serve your predictions, up to the maximum number you set. Refer to the [pricing page](https://cloud.google.com/vertex-ai/pricing#prediction-prices) to understand the costs of autoscaling with multiple nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=f\"{DEPLOYED_NAME}_batch\",\n",
    "    gcs_source=gcs_input_uri,\n",
    "    gcs_destination_prefix=f\"gs://{BUCKET_NAME}\",\n",
    "    instances_format=\"jsonl\",\n",
    "    model_parameters=None,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    starting_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    generate_explanation=True,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "print(batch_predict_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the predictions\n",
    "\n",
    "Next, get the results from the completed batch prediction job.\n",
    "\n",
    "The results are written to the Cloud Storage output bucket you specified in the batch prediction request. You call the method iter_outputs() to get a list of each Cloud Storage file generated with the results. Each file contains one or more prediction requests in a JSON format:\n",
    "\n",
    "- `instance`: The prediction request.\n",
    "- `prediction`: The prediction response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_job = vertex_ai.BatchPredictionJob.list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict_job.output_info.gcs_output_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "filenames = [f\"gs://{blob.bucket.name}/{blob.name}\" for blob in bp_iter_outputs if blob.name.split(\"/\")[-1].startswith(\"explanation.results\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [pd.read_json(filename, lines=True) for filename in filenames]\n",
    "batch_predictions = pd.concat(dataframes)\n",
    "batch_predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictions.explanation = batch_predictions.explanation.apply(lambda x: x['attributions'][0]['featureAttributions']['image']['b64_jpeg'])\n",
    "batch_predictions.explanation = batch_predictions.explanation.apply(lambda x: f'<img src=\"data:image/jpeg;base64,{x}\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictions.instance = batch_predictions.instance.apply(lambda x: x['bytes_inputs']['b64'])\n",
    "batch_predictions.instance = batch_predictions.instance.apply(lambda x: f'<img src=\"data:image/jpeg;base64,{x}\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "batch_predictions.prediction = batch_predictions.prediction.apply(lambda x: tf.nn.softmax(x))\n",
    "batch_predictions.prediction = batch_predictions.prediction.apply(lambda score: \"{} {:.2f}%\".format(class_names[np.argmax(score)], 100 * np.max(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "\n",
    "display(HTML(batch_predictions[['instance', 'explanation', 'prediction']].to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
